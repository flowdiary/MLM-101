{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "908f1d83",
   "metadata": {},
   "source": [
    "# Lecture 83 – Real-Time Inference\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand batch vs real-time inference\n",
    "- Implement WebSocket for streaming predictions\n",
    "- Use FastAPI background tasks\n",
    "- Monitor inference performance\n",
    "- Optimize for low latency\n",
    "\n",
    "## Expected Runtime\n",
    "~5 minutes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7aabc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install fastapi uvicorn websockets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94eb5ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import time\n",
    "from fastapi import FastAPI, WebSocket, BackgroundTasks\n",
    "from typing import List\n",
    "import numpy as np\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1342bee",
   "metadata": {},
   "source": [
    "## 1. Batch vs Real-Time Inference\n",
    "\n",
    "| Aspect | Batch | Real-Time |\n",
    "|--------|-------|----------|\n",
    "| Latency | High (minutes-hours) | Low (ms-seconds) |\n",
    "| Throughput | Very high | Moderate |\n",
    "| Cost | Lower | Higher |\n",
    "| Use Cases | Reports, analytics | User-facing apps |\n",
    "| Complexity | Simpler | More complex |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afa9370",
   "metadata": {},
   "source": [
    "## 2. FastAPI Background Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752656bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Async prediction with background logging\n",
    "from fastapi import FastAPI\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "def log_prediction(request_id: str, result: dict):\n",
    "    \"\"\"Background task for logging.\"\"\"\n",
    "    # Simulate logging to database\n",
    "    time.sleep(0.1)\n",
    "    print(f\"Logged prediction {request_id}: {result}\")\n",
    "\n",
    "@app.post(\"/predict-async\")\n",
    "async def predict_async(data: dict, background_tasks: BackgroundTasks):\n",
    "    \"\"\"Predict with background logging.\"\"\"\n",
    "    request_id = str(time.time())\n",
    "    \n",
    "    # Make prediction (fast)\n",
    "    result = {\"prediction\": \"class_1\", \"confidence\": 0.95}\n",
    "    \n",
    "    # Schedule background task (don't wait)\n",
    "    background_tasks.add_task(log_prediction, request_id, result)\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"✓ Background task example created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0df05e",
   "metadata": {},
   "source": [
    "## 3. WebSocket for Streaming Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adf486a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WebSocket endpoint for streaming\n",
    "@app.websocket(\"/ws/predict\")\n",
    "async def websocket_predict(websocket: WebSocket):\n",
    "    \"\"\"Stream predictions via WebSocket.\"\"\"\n",
    "    await websocket.accept()\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            # Receive data\n",
    "            data = await websocket.receive_json()\n",
    "            \n",
    "            # Simulate prediction\n",
    "            prediction = {\n",
    "                \"class\": np.random.randint(0, 10),\n",
    "                \"confidence\": float(np.random.random()),\n",
    "                \"timestamp\": time.time()\n",
    "            }\n",
    "            \n",
    "            # Send result\n",
    "            await websocket.send_json(prediction)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"WebSocket error: {e}\")\n",
    "    finally:\n",
    "        await websocket.close()\n",
    "\n",
    "print(\"✓ WebSocket endpoint created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e058cd",
   "metadata": {},
   "source": [
    "## 4. WebSocket Client Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07969d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Client code example\n",
    "client_code = '''\n",
    "import asyncio\n",
    "import websockets\n",
    "import json\n",
    "\n",
    "async def stream_predictions():\n",
    "    uri = \"ws://localhost:8000/ws/predict\"\n",
    "    \n",
    "    async with websockets.connect(uri) as websocket:\n",
    "        # Send requests\n",
    "        for i in range(10):\n",
    "            data = {\"image\": [[0]*28]*28}\n",
    "            await websocket.send(json.dumps(data))\n",
    "            \n",
    "            # Receive prediction\n",
    "            response = await websocket.recv()\n",
    "            print(f\"Prediction {i}: {response}\")\n",
    "            \n",
    "            await asyncio.sleep(0.1)\n",
    "\n",
    "# Run\n",
    "asyncio.run(stream_predictions())\n",
    "'''\n",
    "\n",
    "print(\"WebSocket Client Example:\")\n",
    "print(client_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb8b956",
   "metadata": {},
   "source": [
    "## 5. Performance Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7f246a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import Request\n",
    "from prometheus_client import Counter, Histogram, generate_latest\n",
    "import time\n",
    "\n",
    "# Prometheus metrics\n",
    "REQUEST_COUNT = Counter('http_requests_total', 'Total HTTP requests', ['method', 'endpoint'])\n",
    "REQUEST_LATENCY = Histogram('http_request_duration_seconds', 'HTTP request latency')\n",
    "\n",
    "@app.middleware(\"http\")\n",
    "async def metrics_middleware(request: Request, call_next):\n",
    "    \"\"\"Middleware to collect metrics.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Process request\n",
    "    response = await call_next(request)\n",
    "    \n",
    "    # Record metrics\n",
    "    duration = time.time() - start_time\n",
    "    REQUEST_COUNT.labels(method=request.method, endpoint=request.url.path).inc()\n",
    "    REQUEST_LATENCY.observe(duration)\n",
    "    \n",
    "    return response\n",
    "\n",
    "@app.get(\"/metrics\")\n",
    "async def metrics():\n",
    "    \"\"\"Prometheus metrics endpoint.\"\"\"\n",
    "    return generate_latest()\n",
    "\n",
    "print(\"✓ Monitoring middleware created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fe5b0f",
   "metadata": {},
   "source": [
    "## 6. Latency Optimization Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8188cfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimization_tips = '''\n",
    "### Model Optimization\n",
    "1. **Quantization**: Convert FP32 to INT8\n",
    "   - TensorFlow Lite, ONNX Runtime\n",
    "   - 4x smaller, 2-4x faster\n",
    "\n",
    "2. **Pruning**: Remove unnecessary weights\n",
    "   - Can reduce model size by 90%\n",
    "\n",
    "3. **Knowledge Distillation**: Train smaller model\n",
    "   - Teacher-student approach\n",
    "\n",
    "### Serving Optimization\n",
    "1. **Batch predictions**: Group requests\n",
    "   - Higher throughput\n",
    "   - Trade latency for efficiency\n",
    "\n",
    "2. **Model caching**: Keep model in memory\n",
    "   - Load once at startup\n",
    "   - Use global variables\n",
    "\n",
    "3. **GPU acceleration**: Use CUDA\n",
    "   - 10-100x speedup for large models\n",
    "\n",
    "4. **TensorRT / ONNX**: Optimize graph\n",
    "   - Fuse operations\n",
    "   - Hardware-specific optimization\n",
    "\n",
    "### Infrastructure\n",
    "1. **Load balancing**: Distribute requests\n",
    "   - Multiple replicas\n",
    "   - Auto-scaling\n",
    "\n",
    "2. **Caching**: Redis for frequent requests\n",
    "   - Sub-millisecond retrieval\n",
    "\n",
    "3. **CDN**: Edge deployment\n",
    "   - Reduce network latency\n",
    "'''\n",
    "\n",
    "print(optimization_tips)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea34e15",
   "metadata": {},
   "source": [
    "## 7. Batching for Throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d51c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from collections import deque\n",
    "\n",
    "class BatchPredictor:\n",
    "    \"\"\"Batch predictions for higher throughput.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, max_batch_size=32, max_wait_time=0.1):\n",
    "        self.model = model\n",
    "        self.max_batch_size = max_batch_size\n",
    "        self.max_wait_time = max_wait_time\n",
    "        self.queue = deque()\n",
    "        self.lock = asyncio.Lock()\n",
    "    \n",
    "    async def predict(self, data):\n",
    "        \"\"\"Add to batch and wait for result.\"\"\"\n",
    "        future = asyncio.Future()\n",
    "        \n",
    "        async with self.lock:\n",
    "            self.queue.append((data, future))\n",
    "        \n",
    "        # Trigger batch if full\n",
    "        if len(self.queue) >= self.max_batch_size:\n",
    "            asyncio.create_task(self._process_batch())\n",
    "        \n",
    "        return await future\n",
    "    \n",
    "    async def _process_batch(self):\n",
    "        \"\"\"Process accumulated batch.\"\"\"\n",
    "        async with self.lock:\n",
    "            if not self.queue:\n",
    "                return\n",
    "            \n",
    "            batch = list(self.queue)\n",
    "            self.queue.clear()\n",
    "        \n",
    "        # Extract data\n",
    "        data_batch = [d for d, _ in batch]\n",
    "        futures = [f for _, f in batch]\n",
    "        \n",
    "        # Batch predict\n",
    "        results = self.model.predict(np.array(data_batch))\n",
    "        \n",
    "        # Set results\n",
    "        for future, result in zip(futures, results):\n",
    "            future.set_result(result)\n",
    "\n",
    "print(\"✓ Batch predictor created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab942c9",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "✓ Compared batch and real-time inference  \n",
    "✓ Implemented background tasks  \n",
    "✓ Created WebSocket streaming  \n",
    "✓ Added performance monitoring  \n",
    "✓ Learned optimization techniques  \n",
    "\n",
    "### Production Checklist:\n",
    "- [ ] Model quantization\n",
    "- [ ] Request batching\n",
    "- [ ] GPU optimization\n",
    "- [ ] Metrics collection\n",
    "- [ ] Auto-scaling\n",
    "- [ ] Circuit breakers\n",
    "- [ ] Rate limiting\n",
    "\n",
    "**Next**: `06_hands_on_lab_deploy_sentiment_or_cnn.ipynb`"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
