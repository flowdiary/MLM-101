{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f0b4202",
   "metadata": {},
   "source": [
    "# Lecture 83 – Model Serialization for Deployment\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand different model serialization formats (`.h5`, `SavedModel`, `.pkl`)\n",
    "- Learn to save and load TensorFlow/Keras models\n",
    "- Version models with timestamps for production tracking\n",
    "- Serialize preprocessing artifacts (tokenizers, scalers)\n",
    "- Test loaded models to ensure consistency\n",
    "\n",
    "## Expected Runtime\n",
    "~5 minutes (includes small CNN training on Fashion-MNIST subset)\n",
    "\n",
    "## Prerequisites\n",
    "- Python 3.9+\n",
    "- TensorFlow 2.x\n",
    "- NumPy, joblib\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d51c90",
   "metadata": {},
   "source": [
    "## Setup and Environment Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44aa8e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install tensorflow==2.15.0 numpy joblib scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb2b6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import joblib\n",
    "import pickle\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b16392",
   "metadata": {},
   "source": [
    "## 1. Train a Simple CNN on Fashion-MNIST (Subset)\n",
    "\n",
    "We'll train a lightweight CNN for image classification. In production, you'd use a larger dataset, but this demonstrates the serialization workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13ebb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Fashion-MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# Use a small subset for fast training\n",
    "SUBSET_SIZE = 5000\n",
    "x_train_subset = x_train[:SUBSET_SIZE]\n",
    "y_train_subset = y_train[:SUBSET_SIZE]\n",
    "x_test_subset = x_test[:1000]\n",
    "y_test_subset = y_test[:1000]\n",
    "\n",
    "# Normalize and reshape\n",
    "x_train_subset = x_train_subset.astype('float32') / 255.0\n",
    "x_test_subset = x_test_subset.astype('float32') / 255.0\n",
    "x_train_subset = x_train_subset.reshape(-1, 28, 28, 1)\n",
    "x_test_subset = x_test_subset.reshape(-1, 28, 28, 1)\n",
    "\n",
    "print(f\"Training data shape: {x_train_subset.shape}\")\n",
    "print(f\"Test data shape: {x_test_subset.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec1ad6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple CNN model\n",
    "def create_cnn_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = create_cnn_model()\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1691f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model (just 2 epochs for demonstration)\n",
    "history = model.fit(\n",
    "    x_train_subset, y_train_subset,\n",
    "    epochs=2,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e04ad68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(x_test_subset, y_test_subset, verbose=0)\n",
    "print(f\"\\nTest accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317dd770",
   "metadata": {},
   "source": [
    "## 2. Model Serialization – Multiple Formats\n",
    "\n",
    "### 2.1 Save as HDF5 (.h5) – Legacy Format\n",
    "The `.h5` format is widely used but considered legacy. It's simple and compact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ed9b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory\n",
    "models_dir = Path('../models')\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save with timestamp for versioning\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "h5_path = models_dir / f\"fashion_mnist_cnn_{timestamp}.h5\"\n",
    "\n",
    "model.save(h5_path)\n",
    "print(f\"Model saved as HDF5: {h5_path}\")\n",
    "print(f\"File size: {h5_path.stat().st_size / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbd7e2d",
   "metadata": {},
   "source": [
    "### 2.2 Save as SavedModel Format – Recommended for Production\n",
    "The `SavedModel` format is TensorFlow's native format, supporting TensorFlow Serving and deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c67745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as SavedModel (directory format)\n",
    "savedmodel_path = models_dir / f\"fashion_mnist_cnn_savedmodel_{timestamp}\"\n",
    "model.save(savedmodel_path, save_format='tf')\n",
    "print(f\"Model saved as SavedModel: {savedmodel_path}\")\n",
    "\n",
    "# Check directory structure\n",
    "for item in savedmodel_path.rglob('*'):\n",
    "    if item.is_file():\n",
    "        print(f\"  {item.relative_to(savedmodel_path)} - {item.stat().st_size / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800b2efc",
   "metadata": {},
   "source": [
    "### 2.3 Save Preprocessing Artifacts\n",
    "In production, you need to apply the same preprocessing pipeline. Save scalers, tokenizers, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ec50d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple preprocessing configuration\n",
    "preprocessing_config = {\n",
    "    'normalization': 'divide_by_255',\n",
    "    'input_shape': (28, 28, 1),\n",
    "    'num_classes': 10,\n",
    "    'class_names': [\n",
    "        'T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save using joblib (preferred for scikit-learn objects)\n",
    "config_path = models_dir / f\"preprocessing_config_{timestamp}.pkl\"\n",
    "joblib.dump(preprocessing_config, config_path)\n",
    "print(f\"Preprocessing config saved: {config_path}\")\n",
    "\n",
    "# Also save using pickle (alternative)\n",
    "config_path_pickle = models_dir / f\"preprocessing_config_{timestamp}_pickle.pkl\"\n",
    "with open(config_path_pickle, 'wb') as f:\n",
    "    pickle.dump(preprocessing_config, f)\n",
    "print(f\"Preprocessing config saved (pickle): {config_path_pickle}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08941f31",
   "metadata": {},
   "source": [
    "## 3. Model Loading and Validation\n",
    "\n",
    "### 3.1 Load HDF5 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c64364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the .h5 model\n",
    "loaded_h5_model = tf.keras.models.load_model(h5_path)\n",
    "print(\"Model loaded from HDF5 format\")\n",
    "\n",
    "# Verify it works\n",
    "h5_predictions = loaded_h5_model.predict(x_test_subset[:5], verbose=0)\n",
    "print(f\"\\nPrediction shape: {h5_predictions.shape}\")\n",
    "print(f\"Sample predictions (first image): {h5_predictions[0].argmax()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4fe5ae",
   "metadata": {},
   "source": [
    "### 3.2 Load SavedModel Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b503e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SavedModel\n",
    "loaded_savedmodel = tf.keras.models.load_model(savedmodel_path)\n",
    "print(\"Model loaded from SavedModel format\")\n",
    "\n",
    "# Verify predictions match\n",
    "savedmodel_predictions = loaded_savedmodel.predict(x_test_subset[:5], verbose=0)\n",
    "print(f\"\\nPrediction shape: {savedmodel_predictions.shape}\")\n",
    "print(f\"Sample predictions (first image): {savedmodel_predictions[0].argmax()}\")\n",
    "\n",
    "# Assert predictions are identical\n",
    "np.testing.assert_allclose(h5_predictions, savedmodel_predictions, rtol=1e-5)\n",
    "print(\"✓ Both models produce identical predictions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dec0672",
   "metadata": {},
   "source": [
    "### 3.3 Load Preprocessing Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408617de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessing config\n",
    "loaded_config = joblib.load(config_path)\n",
    "print(\"Preprocessing config loaded:\")\n",
    "print(f\"  Normalization: {loaded_config['normalization']}\")\n",
    "print(f\"  Input shape: {loaded_config['input_shape']}\")\n",
    "print(f\"  Classes: {loaded_config['class_names']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bef7d9",
   "metadata": {},
   "source": [
    "## 4. Production-Ready Prediction Function\n",
    "\n",
    "Combine model + preprocessing for a complete inference pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824689cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_array, config):\n",
    "    \"\"\"Apply preprocessing based on saved config.\"\"\"\n",
    "    if config['normalization'] == 'divide_by_255':\n",
    "        image_array = image_array.astype('float32') / 255.0\n",
    "    \n",
    "    # Ensure correct shape\n",
    "    if len(image_array.shape) == 2:\n",
    "        image_array = image_array.reshape(1, 28, 28, 1)\n",
    "    elif len(image_array.shape) == 3:\n",
    "        image_array = image_array.reshape(-1, 28, 28, 1)\n",
    "    \n",
    "    return image_array\n",
    "\n",
    "def predict_with_labels(model, image_array, config):\n",
    "    \"\"\"Complete inference pipeline with class labels.\"\"\"\n",
    "    # Preprocess\n",
    "    processed = preprocess_image(image_array, config)\n",
    "    \n",
    "    # Predict\n",
    "    predictions = model.predict(processed, verbose=0)\n",
    "    \n",
    "    # Get class labels\n",
    "    predicted_classes = predictions.argmax(axis=1)\n",
    "    predicted_labels = [config['class_names'][i] for i in predicted_classes]\n",
    "    \n",
    "    return predicted_labels, predictions\n",
    "\n",
    "# Test the pipeline\n",
    "test_image = x_test[0]  # Raw image (not normalized)\n",
    "labels, probs = predict_with_labels(loaded_savedmodel, test_image, loaded_config)\n",
    "\n",
    "print(f\"Predicted class: {labels[0]}\")\n",
    "print(f\"Confidence: {probs[0].max():.4f}\")\n",
    "print(f\"Actual class: {loaded_config['class_names'][y_test[0]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06180c83",
   "metadata": {},
   "source": [
    "## 5. Model Versioning Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693c794e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model registry metadata file\n",
    "model_metadata = {\n",
    "    'model_name': 'fashion_mnist_cnn',\n",
    "    'version': timestamp,\n",
    "    'framework': 'tensorflow',\n",
    "    'framework_version': tf.__version__,\n",
    "    'model_type': 'cnn',\n",
    "    'dataset': 'fashion_mnist',\n",
    "    'training_samples': SUBSET_SIZE,\n",
    "    'test_accuracy': float(test_acc),\n",
    "    'input_shape': [28, 28, 1],\n",
    "    'output_classes': 10,\n",
    "    'saved_formats': ['h5', 'savedmodel'],\n",
    "    'created_at': datetime.now().isoformat(),\n",
    "    'files': {\n",
    "        'h5_model': str(h5_path.name),\n",
    "        'savedmodel': str(savedmodel_path.name),\n",
    "        'preprocessing': str(config_path.name)\n",
    "    }\n",
    "}\n",
    "\n",
    "import json\n",
    "metadata_path = models_dir / f\"model_metadata_{timestamp}.json\"\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(model_metadata, f, indent=2)\n",
    "\n",
    "print(f\"Model metadata saved: {metadata_path}\")\n",
    "print(\"\\nMetadata content:\")\n",
    "print(json.dumps(model_metadata, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacb6c55",
   "metadata": {},
   "source": [
    "## 6. Testing and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4944d40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_serialization(model_path, test_data, test_labels, is_savedmodel=False):\n",
    "    \"\"\"Test that loaded model produces correct outputs.\"\"\"\n",
    "    # Load model\n",
    "    loaded_model = tf.keras.models.load_model(model_path)\n",
    "    \n",
    "    # Test shape\n",
    "    predictions = loaded_model.predict(test_data[:10], verbose=0)\n",
    "    assert predictions.shape == (10, 10), f\"Expected shape (10, 10), got {predictions.shape}\"\n",
    "    \n",
    "    # Test accuracy\n",
    "    loss, acc = loaded_model.evaluate(test_data, test_labels, verbose=0)\n",
    "    assert acc > 0.5, f\"Model accuracy too low: {acc}\"\n",
    "    \n",
    "    # Test prediction consistency\n",
    "    pred1 = loaded_model.predict(test_data[:5], verbose=0)\n",
    "    pred2 = loaded_model.predict(test_data[:5], verbose=0)\n",
    "    np.testing.assert_allclose(pred1, pred2, rtol=1e-5)\n",
    "    \n",
    "    print(f\"✓ All tests passed for {model_path.name}\")\n",
    "    return True\n",
    "\n",
    "# Run tests\n",
    "test_model_serialization(h5_path, x_test_subset, y_test_subset)\n",
    "test_model_serialization(savedmodel_path, x_test_subset, y_test_subset, is_savedmodel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33d1097",
   "metadata": {},
   "source": [
    "## 7. Production Deployment Checklist\n",
    "\n",
    "### Before deploying your model:\n",
    "\n",
    "- [ ] **Model format**: Use SavedModel for TensorFlow Serving or ONNX for cross-platform\n",
    "- [ ] **Versioning**: Include timestamp or semantic version in filenames\n",
    "- [ ] **Metadata**: Save training config, metrics, and dependencies\n",
    "- [ ] **Preprocessing**: Serialize all preprocessing steps (scalers, tokenizers)\n",
    "- [ ] **Testing**: Validate loaded model produces identical predictions\n",
    "- [ ] **Size optimization**: Consider model quantization or pruning for edge deployment\n",
    "- [ ] **Security**: Scan model files for vulnerabilities (pickle can execute code)\n",
    "- [ ] **Storage**: Use object storage (S3, GCS) with versioning enabled\n",
    "- [ ] **Documentation**: Document input/output specs and example usage\n",
    "\n",
    "### Shell commands for model management:\n",
    "\n",
    "```bash\n",
    "# List saved models\n",
    "ls -lh ../models/\n",
    "\n",
    "# Check SavedModel structure\n",
    "saved_model_cli show --dir ../models/fashion_mnist_cnn_savedmodel_*/\n",
    "\n",
    "# Upload to S3 (AWS)\n",
    "aws s3 cp ../models/ s3://my-models-bucket/fashion-mnist/ --recursive\n",
    "\n",
    "# Upload to GCS (Google Cloud)\n",
    "gsutil -m cp -r ../models/ gs://my-models-bucket/fashion-mnist/\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Extension Ideas\n",
    "\n",
    "1. **Model Quantization**: Convert to TensorFlow Lite for mobile deployment\n",
    "2. **ONNX Export**: Convert model to ONNX format for cross-framework compatibility\n",
    "3. **Model Registry**: Integrate with MLflow or Weights & Biases for tracking\n",
    "4. **A/B Testing**: Save multiple model versions and compare in production\n",
    "5. **Automated Testing**: Create pytest suite that validates model loading\n",
    "6. **Model Signing**: Add cryptographic signatures to verify model integrity\n",
    "\n",
    "---\n",
    "\n",
    "**Next**: `02_serving_fastapi.ipynb` - Learn to serve this model via REST API"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
