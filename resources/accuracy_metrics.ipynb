{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70ea6df1"
      },
      "source": [
        "# Regression and Classification Metrics\n",
        "\n",
        "This notebook demonstrates how to calculate common evaluation metrics for both regression and classification tasks using scikit-learn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fbbf71e"
      },
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you have y_test and y_pred defined from your model training\n",
        "# Replace with your actual y_test and y_pred\n",
        "# Example dummy data:\n",
        "y_test = np.random.rand(100)\n",
        "y_pred = np.random.rand(100)\n",
        "\n",
        "y_test_clf = np.random.randint(0, 2, 100)\n",
        "y_pred_clf = np.random.randint(0, 2, 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c960fe9"
      },
      "source": [
        "## Regression Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7905539e"
      },
      "source": [
        "### 1. R-Squared (RÂ²)\n",
        "R-Squared tells us how well the model explains the data. A value closer to 1 means the model is a good fit; a value closer to 0 means it's not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02a12467"
      },
      "source": [
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"R-Squared: {r2}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1660f39"
      },
      "source": [
        "### 2. Mean Squared Error (MSE)\n",
        "MSE shows how far off the predictions are from the actual values. It squares the errors, so bigger mistakes have a larger impact."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "119a3994"
      },
      "source": [
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27d599f8"
      },
      "source": [
        "### 3. Mean Absolute Error (MAE)\n",
        "MAE measures the average of the absolute differences between predicted and actual values. It tells how far off, on average, your predictions are."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9b1dce3e"
      },
      "source": [
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "199ce7b9"
      },
      "source": [
        "### 4. Root Mean Squared Error (RMSE)\n",
        "RMSE is just like MSE, but it takes the square root of the error, bringing it back to the same units as the target variable. It punishes larger errors more."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8af4fd6"
      },
      "source": [
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15d8fb64"
      },
      "source": [
        "## Classification Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8bdead1"
      },
      "source": [
        "### 1. Accuracy\n",
        "Explanation: Accuracy measures how many predictions were correct out of all predictions. It's the most basic classification metric."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66699aff"
      },
      "source": [
        "accuracy = accuracy_score(y_test_clf, y_pred_clf)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b7bd610"
      },
      "source": [
        "### 2. Precision\n",
        "Explanation: Precision tells you how many of the predicted positive cases were actually positive. It's important when the cost of false positives is high."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7b3aa8c"
      },
      "source": [
        "precision = precision_score(y_test_clf, y_pred_clf)\n",
        "print(f\"Precision: {precision:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30f335e9"
      },
      "source": [
        "### 3. Recall\n",
        "Explanation: Recall measures how many of the actual positive cases were correctly identified by the model. It's important when the cost of false negatives is high."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1d3fbad9"
      },
      "source": [
        "recall = recall_score(y_test_clf, y_pred_clf)\n",
        "print(f\"Recall: {recall:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f927c6f8"
      },
      "source": [
        "### 4. F1-Score\n",
        "Explanation: F1-Score is the harmonic mean of precision and recall. It balances precision and recall when you need a single metric to evaluate your model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "250643e3"
      },
      "source": [
        "f1 = f1_score(y_test_clf, y_pred_clf)\n",
        "print(f\"F1-Score: {f1:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea7f45f6"
      },
      "source": [
        "### 5. Confusion Matrix\n",
        "Explanation: A confusion matrix shows the counts of true positives, true negatives, false positives, and false negatives. It helps understand how well the model is performing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ff01e45b"
      },
      "source": [
        "conf_matrix = confusion_matrix(y_test_clf, y_pred_clf)\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}