{"cells":[{"cell_type":"markdown","metadata":{"id":"ZNiLpQZeLmIF"},"source":["# Lecture 69: Sentiment Analysis with NLP\n","\n","This notebook demonstrates how to build a **sentiment analysis model** using Scikit-learn to classify movie reviews as positive or negative. We'll use the IMDb dataset from the `datasets` library, preprocess the text data, extract features using TF-IDF, train a logistic regression model, and evaluate its performance. The steps include:\n","\n","- Loading and exploring the IMDb dataset\n","- Preprocessing text (cleaning, tokenization, lemmatization)\n","- Converting text to numerical features with TF-IDF\n","- Training a logistic regression model\n","- Evaluating the model with accuracy, precision, recall, and F1-score\n","- Making predictions on sample texts\n","\n","This approach is a simple yet effective way to perform sentiment analysis using traditional NLP techniques."]},{"cell_type":"markdown","metadata":{"id":"fxIoOkzYLmIG"},"source":["## Setup and Imports\n","\n","Let's import the necessary libraries and set up the environment."]},{"cell_type":"code","source":["%%capture\n","!pip install -U datasets"],"metadata":{"id":"i7wFfsenL4eN","executionInfo":{"status":"ok","timestamp":1749219435825,"user_tz":-60,"elapsed":4763,"user":{"displayName":"Abdullahi Ahmad","userId":"03775884294431603545"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dK2g_5VHLmIH","executionInfo":{"status":"ok","timestamp":1749219945416,"user_tz":-60,"elapsed":3255,"user":{"displayName":"Abdullahi Ahmad","userId":"03775884294431603545"}},"outputId":"d6f47af0-2a52-47ad-f67a-3707686c45e5"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import re\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n","from datasets import load_dataset\n","\n","# Download required NLTK data\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('punkt_tab')\n","# Set random seed for reproducibility\n","np.random.seed(42)"]},{"cell_type":"markdown","metadata":{"id":"VtweVOROLmII"},"source":["## Loading and Exploring the IMDb Dataset\n","\n","We'll use the IMDb dataset from the `datasets` library, which contains 50,000 movie reviews labeled as positive (1) or negative (0). We'll load a subset to keep the computation manageable."]},{"source":["dataset = load_dataset(\"imdb\")\n","\n","\n","# Convert to pandas DataFrame\n","df_train = pd.DataFrame(dataset['train']).sample(10000, random_state=42)  # Subset of 10,000 reviews\n","# Accessing the 'test' split should now work\n","df_test = pd.DataFrame(dataset['test']).sample(2000, random_state=42)    # Subset of 2,000 reviews\n","\n","# Extract texts and labels\n","X_train = df_train['text']\n","y_train = df_train['label']\n","X_test = df_test['text']\n","y_test = df_test['label']\n","\n","print(f\"Training data shape: {X_train.shape}\")\n","print(f\"Test data shape: {X_test.shape}\")\n","\n","# Display sample reviews\n","print(\"\\nSample Reviews:\")\n","for i in range(2):\n","    print(f\"Review {i+1}: {X_train.iloc[i][:100]}...\\nLabel: {'Positive' if y_train.iloc[i] == 1 else 'Negative'}\\n\")"],"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f7NI_rKrqs4h","executionInfo":{"status":"ok","timestamp":1749221003563,"user_tz":-60,"elapsed":9111,"user":{"displayName":"Abdullahi Ahmad","userId":"03775884294431603545"}},"outputId":"3dfff54e-fb16-40ec-ea65-60e2e4df583d"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Training data shape: (10000,)\n","Test data shape: (2000,)\n","\n","Sample Reviews:\n","Review 1: Dumb is as dumb does, in this thoroughly uninteresting, supposed black comedy. Essentially what star...\n","Label: Negative\n","\n","Review 2: I dug out from my garage some old musicals and this is another one of my favorites. It was written b...\n","Label: Positive\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"JQ1PEiIjLmIJ"},"source":["## Text Preprocessing\n","\n","We'll preprocess the text by:\n","- Converting to lowercase\n","- Removing special characters and numbers\n","- Tokenizing the text\n","- Removing stop words\n","- Applying lemmatization to reduce words to their base form"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gt-dotPjLmIK","executionInfo":{"status":"ok","timestamp":1749221077221,"user_tz":-60,"elapsed":14512,"user":{"displayName":"Abdullahi Ahmad","userId":"03775884294431603545"}},"outputId":"c88c43d0-9d20-4b61-f235-170f181ea4ba"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sample Preprocessed Review:\n","dumb dumb thoroughly uninteresting supposed black comedy essentially start chris klein trying maintain low profile eventually morphs uninspired version three amigo without laugh order black comedy wor...\n"]}],"source":["stop_words = set(stopwords.words('english'))\n","lemmatizer = WordNetLemmatizer()\n","\n","def preprocess_text(text):\n","    # Convert to lowercase\n","    text = text.lower()\n","    # Remove special characters and numbers\n","    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n","    # Tokenize\n","    tokens = word_tokenize(text)\n","    # Remove stop words and lemmatize\n","    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n","    return ' '.join(tokens)\n","\n","# Apply preprocessing\n","X_train_processed = X_train.apply(preprocess_text)\n","X_test_processed = X_test.apply(preprocess_text)\n","\n","print(\"Sample Preprocessed Review:\")\n","print(X_train_processed.iloc[0][:200] + \"...\")"]},{"cell_type":"markdown","metadata":{"id":"o39iCuNbLmIK"},"source":["## Feature Extraction with TF-IDF\n","\n","We'll convert the preprocessed text into numerical features using TF-IDF (Term Frequency-Inverse Document Frequency), which weighs words based on their importance in the document and across the corpus."]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ge9jK_iALmIL","executionInfo":{"status":"ok","timestamp":1749221117058,"user_tz":-60,"elapsed":5716,"user":{"displayName":"Abdullahi Ahmad","userId":"03775884294431603545"}},"outputId":"13becf89-3a30-4939-91bc-7136d00405bc"},"outputs":[{"output_type":"stream","name":"stdout","text":["TF-IDF Training data shape: (10000, 5000)\n","TF-IDF Test data shape: (2000, 5000)\n"]}],"source":["# Initialize TF-IDF vectorizer\n","tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n","\n","# Fit and transform the training data\n","X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_processed)\n","\n","# Transform the test data\n","X_test_tfidf = tfidf_vectorizer.transform(X_test_processed)\n","\n","print(f\"TF-IDF Training data shape: {X_train_tfidf.shape}\")\n","print(f\"TF-IDF Test data shape: {X_test_tfidf.shape}\")"]},{"cell_type":"markdown","metadata":{"id":"YsuiYG5JLmIM"},"source":["## Training the Sentiment Analysis Model\n","\n","We'll train a logistic regression model, which is effective for binary classification tasks like sentiment analysis."]},{"cell_type":"code","execution_count":19,"metadata":{"id":"ywjTZ0FCLmIM","executionInfo":{"status":"ok","timestamp":1749221146412,"user_tz":-60,"elapsed":587,"user":{"displayName":"Abdullahi Ahmad","userId":"03775884294431603545"}}},"outputs":[],"source":["# Initialize and train logistic regression model\n","model = LogisticRegression(max_iter=1000, random_state=42)\n","model.fit(X_train_tfidf, y_train)\n","\n","# Make predictions on test set\n","y_pred = model.predict(X_test_tfidf)"]},{"cell_type":"markdown","metadata":{"id":"IJgB68CcLmIM"},"source":["## Evaluating the Model\n","\n","We'll evaluate the model using accuracy, precision, recall, F1-score, and a detailed classification report."]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ufXKidKnLmIN","executionInfo":{"status":"ok","timestamp":1749221156689,"user_tz":-60,"elapsed":270,"user":{"displayName":"Abdullahi Ahmad","userId":"03775884294431603545"}},"outputId":"87716897-3fcf-472c-f11b-58a8829aad42"},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.8715\n","Precision: 0.8576\n","Recall: 0.8781\n","F1-Score: 0.8677\n","\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","    Negative       0.88      0.87      0.88      1040\n","    Positive       0.86      0.88      0.87       960\n","\n","    accuracy                           0.87      2000\n","   macro avg       0.87      0.87      0.87      2000\n","weighted avg       0.87      0.87      0.87      2000\n","\n"]}],"source":["# Calculate evaluation metrics\n","accuracy = accuracy_score(y_test, y_pred)\n","precision = precision_score(y_test, y_pred)\n","recall = recall_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred)\n","\n","print(f\"Accuracy: {accuracy:.4f}\")\n","print(f\"Precision: {precision:.4f}\")\n","print(f\"Recall: {recall:.4f}\")\n","print(f\"F1-Score: {f1:.4f}\")\n","\n","# Detailed classification report\n","print(\"\\nClassification Report:\")\n","print(classification_report(y_test, y_pred, target_names=['Negative', 'Positive']))"]},{"cell_type":"markdown","metadata":{"id":"nts5e4jjLmIN"},"source":["## Making Predictions on Sample Texts\n","\n","Let's test the model on some custom review texts to see how it predicts sentiment."]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zFoLeJFqLmIN","executionInfo":{"status":"ok","timestamp":1749221208429,"user_tz":-60,"elapsed":255,"user":{"displayName":"Abdullahi Ahmad","userId":"03775884294431603545"}},"outputId":"da73b64d-8437-40e4-e3eb-fcdf197356de"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Sample Text Predictions:\n","Text: This movie was absolutely fantastic! The acting was superb and the plot kept me engaged throughout....\n","Predicted Sentiment: Positive\n","\n","Text: I was really disappointed with this film. The story was boring and the characters were unlikable....\n","Predicted Sentiment: Negative\n","\n","Text: An average movie with some good moments but nothing special overall....\n","Predicted Sentiment: Positive\n","\n"]}],"source":["# Sample texts for prediction\n","sample_texts = [\n","    \"This movie was absolutely fantastic! The acting was superb and the plot kept me engaged throughout.\",\n","    \"I was really disappointed with this film. The story was boring and the characters were unlikable.\",\n","    \"An average movie with some good moments but nothing special overall.\"\n","]\n","\n","# Preprocess and transform sample texts\n","sample_texts_processed = [preprocess_text(text) for text in sample_texts]\n","sample_tfidf = tfidf_vectorizer.transform(sample_texts_processed)\n","\n","# Predict sentiment\n","sample_predictions = model.predict(sample_tfidf)\n","\n","# Display predictions\n","print(\"\\nSample Text Predictions:\")\n","for text, pred in zip(sample_texts, sample_predictions):\n","    print(f\"Text: {text[:100]}...\\nPredicted Sentiment: {'Positive' if pred == 1 else 'Negative'}\\n\")"]},{"cell_type":"markdown","metadata":{"id":"l3NnAFBcLmIN"},"source":["## Explanation\n","\n","- **Dataset**: Used a subset of the IMDb dataset (10,000 training, 2,000 test reviews) labeled as positive or negative.\n","- **Preprocessing**: Applied text cleaning, tokenization, stop word removal, and lemmatization to standardize the text.\n","- **Feature Extraction**: Used TF-IDF to convert text into numerical features, capturing word importance with up to 5,000 features and bigrams.\n","- **Model**: Trained a logistic regression model, which is effective for binary classification and interpretable.\n","- **Evaluation**: Assessed performance with accuracy, precision, recall, and F1-score, providing a comprehensive view of model quality.\n","- **Predictions**: Tested the model on custom texts to demonstrate real-world applicability.\n","\n","To extend this work, consider:\n","- Using other models (e.g., SVM, Naive Bayes, or deep learning with LSTM)\n","- Incorporating word embeddings (e.g., Word2Vec, GloVe) for better semantic understanding\n","- Tuning hyperparameters (e.g., TF-IDF max_features, n-grams, or regularization in logistic regression)\n","- Handling imbalanced data or expanding to multi-class sentiment (e.g., neutral)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}