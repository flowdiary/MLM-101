{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation (RAG) with LangChain, Pinecone, and ChromaDB for book.pdf\n",
    "\n",
    "This notebook demonstrates **Retrieval-Augmented Generation (RAG)** using **LangChain** with **Pinecone** and **ChromaDB** as vector databases, processing a user-provided PDF document (`book.pdf`). The RAG system extracts text from the PDF, chunks it, creates embeddings, stores them in both vector databases, and answers queries based on the document content. It also compares the performance of Pinecone and ChromaDB.\n",
    "\n",
    "## Objectives\n",
    "- Extract text from `book.pdf`\n",
    "- Chunk the text and create vector embeddings\n",
    "- Store embeddings in Pinecone and ChromaDB\n",
    "- Set up RAG pipelines for question answering\n",
    "- Compare Pinecone and ChromaDB performance\n",
    "\n",
    "## Prerequisites\n",
    "- `book.pdf` in the working directory\n",
    "- Pinecone account and API key (free tier available)\n",
    "- Ollama installed locally with a model (e.g., `llama3`)\n",
    "- Required libraries: `langchain`, `pinecone-client`, `chromadb`, `pypdf`, `sentence-transformers`, `langchain-community`, `langchain-ollama`\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "Install and import the necessary libraries. Replace the Pinecone API key and environment with your credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -qU langchain langchain-community langchain-ollama pinecone-client chromadb sentence-transformers pypdf\n",
    "\n",
    "import os\n",
    "#import pinecone\n",
    "import chromadb\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Pinecone, Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_groq import ChatGroq\n",
    "import time\n",
    "\n",
    "# Set Groq API key\n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load and Preprocess book.pdf\n",
    "\n",
    "We'll use `PyPDFLoader` to extract text from `book.pdf` and split it into chunks suitable for retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of document chunks: 78\n",
      "Sample chunk:\n",
      "AI For Everyone\n",
      "Om Prabhu\n",
      "19D170018\n",
      "Undergraduate, Department of Energy Science and Engineering\n",
      "Indian Institute of Technology Bombay\n",
      "Last updated January 31, 2021\n",
      "NOTE: This document is a brief compi...\n"
     ]
    }
   ],
   "source": [
    "# Load PDF\n",
    "loader = PyPDFLoader(\"data/AI for Everyone.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Split into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Number of document chunks: {len(docs)}\")\n",
    "print(\"Sample chunk:\")\n",
    "print(docs[0].page_content[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Embeddings\n",
    "\n",
    "We'll use Hugging Face's `all-MiniLM-L6-v2` model to create 384-dimensional embeddings for the document chunks, compatible with both Pinecone and ChromaDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimension: 384\n",
      "Sample embedding (first 5 values): [0.03519676998257637, 0.07600128650665283, 0.023663559928536415, 0.14537909626960754, -0.01117265410721302]\n"
     ]
    }
   ],
   "source": [
    "# Initialize Hugging Face embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')\n",
    "\n",
    "# Test embedding\n",
    "sample_text = \"This is a test sentence from the book.\"\n",
    "sample_embedding = embeddings.embed_query(sample_text)\n",
    "print(f\"Embedding dimension: {len(sample_embedding)}\")\n",
    "print(f\"Sample embedding (first 5 values): {sample_embedding[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Store Embeddings in Pinecone\n",
    "\n",
    "We'll create a Pinecone index, store the document embeddings, and set up a LangChain vector store for retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create Pinecone index\n",
    "# index_name = 'book-rag'\n",
    "# if index_name not in pinecone.list_indexes():\n",
    "#     pinecone.create_index(index_name, dimension=384, metric='cosine')\n",
    "\n",
    "# # Initialize Pinecone index\n",
    "# index = pinecone.Index(index_name)\n",
    "\n",
    "# # Store documents in Pinecone\n",
    "# pinecone_store = Pinecone.from_documents(docs, embeddings, index_name=index_name)\n",
    "\n",
    "# print(f\"Pinecone index '{index_name}' created and populated with {len(docs)} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Store Embeddings in ChromaDB\n",
    "\n",
    "We'll create a ChromaDB collection, store the document embeddings, and set up a LangChain vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Database error: error returned from database: (code: 14) unable to open database file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize ChromaDB client\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m chroma_client \u001b[38;5;241m=\u001b[39m chromadb\u001b[38;5;241m.\u001b[39mPersistentClient(path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./chroma_db\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Store documents in ChromaDB\u001b[39;00m\n\u001b[1;32m      5\u001b[0m chroma_store \u001b[38;5;241m=\u001b[39m Chroma\u001b[38;5;241m.\u001b[39mfrom_documents(\n\u001b[1;32m      6\u001b[0m     docs,\n\u001b[1;32m      7\u001b[0m     embeddings,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     persist_directory\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./chroma_db\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     11\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/chromadb/__init__.py:167\u001b[0m, in \u001b[0;36mPersistentClient\u001b[0;34m(path, settings, tenant, database)\u001b[0m\n\u001b[1;32m    164\u001b[0m tenant \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(tenant)\n\u001b[1;32m    165\u001b[0m database \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(database)\n\u001b[0;32m--> 167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ClientCreator(tenant\u001b[38;5;241m=\u001b[39mtenant, database\u001b[38;5;241m=\u001b[39mdatabase, settings\u001b[38;5;241m=\u001b[39msettings)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/chromadb/api/client.py:96\u001b[0m, in \u001b[0;36mClient.__init__\u001b[0;34m(self, tenant, database, settings)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Create an admin client for verifying that databases and tenants exist\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_admin_client \u001b[38;5;241m=\u001b[39m AdminClient\u001b[38;5;241m.\u001b[39mfrom_system(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_system)\n\u001b[0;32m---> 96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_tenant_database(tenant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtenant, database\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatabase)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_submit_client_start_event()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/chromadb/api/client.py:478\u001b[0m, in \u001b[0;36mClient._validate_tenant_database\u001b[0;34m(self, tenant, database)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;66;03m# Propagate ChromaErrors\u001b[39;00m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ChromaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 478\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    481\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not connect to tenant \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtenant\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Are you sure it exists?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    482\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/chromadb/api/client.py:471\u001b[0m, in \u001b[0;36mClient._validate_tenant_database\u001b[0;34m(self, tenant, database)\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_tenant_database\u001b[39m(\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28mself\u001b[39m, tenant: \u001b[38;5;28mstr\u001b[39m, database: \u001b[38;5;28mstr\u001b[39m\n\u001b[1;32m    469\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 471\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_admin_client\u001b[38;5;241m.\u001b[39mget_tenant(name\u001b[38;5;241m=\u001b[39mtenant)\n\u001b[1;32m    472\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mConnectError:\n\u001b[1;32m    473\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    474\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not connect to a Chroma server. Are you sure it is running?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    475\u001b[0m         )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/chromadb/api/client.py:528\u001b[0m, in \u001b[0;36mAdminClient.get_tenant\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    526\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_tenant\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tenant:\n\u001b[0;32m--> 528\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_server\u001b[38;5;241m.\u001b[39mget_tenant(name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/chromadb/api/rust.py:165\u001b[0m, in \u001b[0;36mRustBindingsAPI.get_tenant\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_tenant\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tenant:\n\u001b[0;32m--> 165\u001b[0m     tenant \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbindings\u001b[38;5;241m.\u001b[39mget_tenant(name)\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Tenant(name\u001b[38;5;241m=\u001b[39mtenant\u001b[38;5;241m.\u001b[39mname)\n",
      "\u001b[0;31mInternalError\u001b[0m: Database error: error returned from database: (code: 14) unable to open database file"
     ]
    }
   ],
   "source": [
    "# Initialize ChromaDB client\n",
    "chroma_client = chromadb.PersistentClient(path='./chroma_db')\n",
    "\n",
    "# Store documents in ChromaDB\n",
    "chroma_store = Chroma.from_documents(\n",
    "    docs,\n",
    "    embeddings,\n",
    "    client=chroma_client,\n",
    "    collection_name='book-rag',\n",
    "    persist_directory='./chroma_db'\n",
    ")\n",
    "\n",
    "print(f\"ChromaDB collection 'book-rag' created and populated with {len(docs)} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Set Up RAG Pipeline\n",
    "\n",
    "We'll use LangChain's `RetrievalQA` chain with an Ollama-hosted LLM (`llama3`) to create RAG pipelines for both Pinecone and ChromaDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG pipelines initialized for Pinecone and ChromaDB.\n"
     ]
    }
   ],
   "source": [
    "# Initialize LLM (Ollama with llama3)\n",
    "#llm = OllamaLLM(model='llama3')\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "# Create RetrievalQA chains\n",
    "# pinecone_qa = RetrievalQA.from_chain_type(\n",
    "#     llm=llm,\n",
    "#     chain_type='stuff',\n",
    "#     retriever=pinecone_store.as_retriever(search_kwargs={'k': 3})\n",
    "# )\n",
    "\n",
    "chroma_qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type='stuff',\n",
    "    retriever=chroma_store.as_retriever(search_kwargs={'k': 3})\n",
    ")\n",
    "\n",
    "print(\"RAG pipelines initialized for Pinecone and ChromaDB.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Test RAG Pipelines\n",
    "\n",
    "We'll test both RAG pipelines with sample queries relevant to the content of `book.pdf`. Since I don't know the exact content of your PDF, I'll provide generic queries that you can modify based on your document's topics. We'll measure response time and compare outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: What is the main topic of the book?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_6/8cmwyh4d5r19m_2bpjj83t_r0000gn/T/ipykernel_65033/2882606434.py:21: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  chroma_result = chroma_qa.run(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChromaDB Response (Time: 1.98s):\n",
      "The main topic of the book \"AI For Everyone\" is Artificial Intelligence (AI).\n",
      "\n",
      "Query: What is the main difference between Artificial Narrow Intelligence (ANI) and Artificial General Intelligence (AGI)?\n",
      "ChromaDB Response (Time: 0.76s):\n",
      "The main difference between Artificial Narrow Intelligence (ANI) and Artificial General Intelligence (AGI) is their scope and capabilities.\n",
      "\n",
      "Artificial Narrow Intelligence (ANI) is a type of AI that is designed to perform a specific task or a narrow set of tasks. It can excel in a particular area, such as:\n",
      "\n",
      "- Recognizing images\n",
      "- Translating languages\n",
      "- Playing chess\n",
      "- Driving a car\n",
      "\n",
      "ANI is incredibly valuable in specific industries due to its narrow application, but it is limited to a particular domain.\n",
      "\n",
      "On the other hand, Artificial General Intelligence (AGI) is a type of AI that is designed to perform any intellectual task that a human can. It has the ability to reason, learn, and apply its knowledge across a wide range of tasks, similar to human intelligence. AGI would be capable of:\n",
      "\n",
      "- Learning from experience\n",
      "- Reasoning and problem-solving\n",
      "- Understanding natural language\n",
      "- Applying knowledge across multiple domains\n",
      "\n",
      "In other words, AGI is a more general and versatile type of intelligence that can adapt to new situations and tasks, whereas ANI is a more specialized and narrow type of intelligence.\n",
      "\n",
      "Query: What is one real-life example of how supervised learning is used?\n",
      "ChromaDB Response (Time: 0.57s):\n",
      "One real-life example of supervised learning is in the online advertising industry. AI algorithms use supervised learning to analyze advertisement details and user information to predict whether a user will click on an ad or not. This helps to show users a certain set of advertisements online.\n"
     ]
    }
   ],
   "source": [
    "# Sample queries (modify based on your book.pdf content)\n",
    "queries = [\n",
    "    \"What is the main topic of the book?\",\n",
    "    \"What is the main difference between Artificial Narrow Intelligence (ANI) and Artificial General Intelligence (AGI)?\",\n",
    "    \"What is one real-life example of how supervised learning is used?\"\n",
    "]\n",
    "\n",
    "# Test and compare\n",
    "for query in queries:\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    \n",
    "    # # Pinecone\n",
    "    # start_time = time.time()\n",
    "    # pinecone_result = pinecone_qa.run(query)\n",
    "    # pinecone_time = time.time() - start_time\n",
    "    # print(f\"Pinecone Response (Time: {pinecone_time:.2f}s):\")\n",
    "    # print(pinecone_result)\n",
    "    \n",
    "    # ChromaDB\n",
    "    start_time = time.time()\n",
    "    chroma_result = chroma_qa.run(query)\n",
    "    chroma_time = time.time() - start_time\n",
    "    print(f\"ChromaDB Response (Time: {chroma_time:.2f}s):\")\n",
    "    print(chroma_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Comparison of Pinecone and ChromaDB\n",
    "\n",
    "### Pinecone\n",
    "- **Pros**:\n",
    "  - Fully managed, scalable vector database\n",
    "  - Optimized for real-time similarity search\n",
    "  - Easy integration with LangChain\n",
    "  - Ideal for production with high throughput\n",
    "- **Cons**:\n",
    "  - Requires an account and API key; free tier limited to 100,000 vectors\n",
    "  - Costly for large-scale use\n",
    "- **Performance**: Typically faster for cloud-based retrieval (~500ms for small indexes)\n",
    "\n",
    "### ChromaDB\n",
    "- **Pros**:\n",
    "  - Open-source and free\n",
    "  - Easy to set up locally or in Docker\n",
    "  - Persists data locally\n",
    "  - Great for prototyping and development\n",
    "- **Cons**:\n",
    "  - Self-hosted, requiring infrastructure management\n",
    "  - Less optimized for large-scale production\n",
    "- **Performance**: Slightly slower for large datasets; performance depends on local hardware\n",
    "\n",
    "## Recommendations\n",
    "- **Use Pinecone** for production-grade applications requiring scalability and managed infrastructure.\n",
    "- **Use ChromaDB** for prototyping, local testing, or projects where self-hosting is preferred.\n",
    "\n",
    "## Notes\n",
    "- If `book.pdf` is large, adjust `chunk_size` and `chunk_overlap` to balance retrieval accuracy and performance.\n",
    "- If the PDF contains scanned images, use an OCR tool like `pytesseract` (let me know if you need help with this).\n",
    "- Response times depend on network latency (Pinecone), hardware (ChromaDB), and LLM performance (Ollama).\n",
    "\n",
    "## Cleanup\n",
    "\n",
    "Delete the Pinecone index and ChromaDB collection to free resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup Pinecone\n",
    "# if index_name in pinecone.list_indexes():\n",
    "#     pinecone.delete_index(index_name)\n",
    "#     print(f\"Deleted Pinecone index '{index_name}'.\")\n",
    "\n",
    "# Cleanup ChromaDB\n",
    "chroma_client.delete_collection('book-rag')\n",
    "print(\"Deleted ChromaDB collection 'book-rag'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation\n",
    "\n",
    "- **PDF Loading and Splitting**: Used `PyPDFLoader` to extract text from `book.pdf` and `RecursiveCharacterTextSplitter` to create manageable chunks.\n",
    "- **Embeddings**: Employed Hugging Face's `all-MiniLM-L6-v2` for 384-dimensional embeddings.\n",
    "- **Pinecone**: Created a cloud-based index, stored embeddings, and set up a `Pinecone` vector store.\n",
    "- **ChromaDB**: Created a local collection, stored embeddings, and set up a `Chroma` vector store.\n",
    "- **RAG Pipeline**: Used `RetrievalQA` with an Ollama-hosted `llama3` LLM to answer queries, retrieving the top-3 relevant chunks.\n",
    "- **Comparison**: Evaluated Pinecone and ChromaDB based on setup, performance, and use cases.\n",
    "\n",
    "## Next Steps\n",
    "- **Modify Queries**: Replace the sample queries with ones specific to `book.pdf` content (e.g., key concepts, chapters, or examples).\n",
    "- **Handle Large PDFs**: If `book.pdf` is large, test with smaller `chunk_size` (e.g., 500) or increase `k` in `search_kwargs`.\n",
    "- **OCR for Scanned PDFs**: If the PDF is scanned, install `pytesseract` and use `pdf2image` for text extraction (I can provide a modified notebook).\n",
    "- **Advanced RAG**: Add prompt templates, hybrid search, or reranking for better results.\n",
    "- **Deployment**: Create a UI with Chainlit or deploy the RAG system as an API.\n",
    "\n",
    "If you encounter issues (e.g., PDF extraction fails, Pinecone setup errors, or specific content queries), let me know, and I can provide tailored solutions!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
