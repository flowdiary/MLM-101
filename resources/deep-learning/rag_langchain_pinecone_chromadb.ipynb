{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation (RAG) with LangChain, Pinecone, and ChromaDB\n",
    "\n",
    "This notebook demonstrates how to implement **Retrieval-Augmented Generation (RAG)** using **LangChain** with **Pinecone** and **ChromaDB** as vector databases. RAG combines retrieval-based and generation-based methods to enhance the performance of large language models (LLMs) by providing relevant context from a knowledge base. We'll use a sample text file as our knowledge base, create vector embeddings, store them in both Pinecone and ChromaDB, and set up RAG pipelines to answer queries. The notebook also compares the performance of both vector databases.\n",
    "\n",
    "## Objectives\n",
    "- Load and preprocess a sample document\n",
    "- Create vector embeddings using Hugging Face models\n",
    "- Store embeddings in Pinecone and ChromaDB\n",
    "- Set up RAG pipelines with LangChain\n",
    "- Compare Pinecone and ChromaDB for a question-answering task\n",
    "\n",
    "## Prerequisites\n",
    "- Python 3.8+\n",
    "- Pinecone account and API key (free tier available)\n",[](https://blog.gopenai.com/primer-on-vector-databases-and-retrieval-augmented-generation-rag-using-langchain-pinecone-37a27fb10546?gi=c3a16f2e233d)
    "- Ollama installed locally with a model (e.g., `llama3`)\n",
    "- Required libraries: `langchain`, `pinecone-client`, `chromadb`, `huggingface_hub`, `sentence-transformers`, `langchain-community`, `langchain-ollama`\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "Install and import the necessary libraries. Ensure you have a Pinecone API key and Ollama running locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU langchain langchain-community langchain-ollama pinecone-client chromadb sentence-transformers huggingface_hub\n",
    "\n",
    "import os\n",
    "import pinecone\n",
    "import chromadb\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Pinecone, Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.schema import Document\n",
    "import time\n",
    "\n",
    "# Set Pinecone API key and environment\n",
    "os.environ['PINECONE_API_KEY'] = 'your-pinecone-api-key'  # Replace with your Pinecone API key\n",
    "PINECONE_ENV = 'your-pinecone-environment'  # Replace with your Pinecone environment (e.g., 'us-west1-gcp')\n",
    "\n",
    "# Initialize Pinecone\n",
    "pinecone.init(api_key=os.environ['PINECONE_API_KEY'], environment=PINECONE_ENV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load and Preprocess Documents\n",
    "\n",
    "We'll create a sample text file as our knowledge base and load it using LangChain's `TextLoader`. The text will be split into chunks to optimize retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample text file\n",
    "sample_text = \"\"\"## Introduction to Machine Learning\n",
    "\n",
    "Machine learning (ML) is a subset of artificial intelligence that enables systems to learn from data and improve without explicit programming. It involves algorithms that identify patterns and make predictions.\n",
    "\n",
    "## Types of Machine Learning\n",
    "\n",
    "### Supervised Learning\n",
    "Supervised learning uses labeled data to train models. Examples include classification (e.g., spam detection) and regression (e.g., house price prediction).\n",
    "\n",
    "### Unsupervised Learning\n",
    "Unsupervised learning works with unlabeled data to find hidden patterns. Examples include clustering (e.g., customer segmentation) and dimensionality reduction.\n",
    "\n",
    "### Reinforcement Learning\n",
    "Reinforcement learning involves agents learning through rewards and penalties in an environment. It is used in robotics and game playing.\n",
    "\n",
    "## Applications\n",
    "\n",
    "ML is used in healthcare (disease prediction), finance (fraud detection), and technology (recommendation systems).\n",
    "\"\"\"\n",
    "\n",
    "with open('ml_intro.txt', 'w') as f:\n",
    "    f.write(sample_text)\n",
    "\n",
    "# Load and split the document\n",
    "loader = TextLoader('ml_intro.txt')\n",
    "documents = loader.load()\n",
    "\n",
    "# Split into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Number of document chunks: {len(docs)}\")\n",
    "print(\"Sample chunk:\")\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Embeddings\n",
    "\n",
    "We'll use Hugging Face's `all-MiniLM-L6-v2` model to create embeddings for the document chunks. This model maps text to a 384-dimensional vector space, suitable for both Pinecone and ChromaDB."[](https://battox.medium.com/retrieval-augmented-generation-in-very-few-lines-of-code-ba9ae02425eb)
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Hugging Face embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')\n",
    "\n",
    "# Test embedding\n",
    "sample_text = \"Machine learning is a subset of AI.\"\n",
    "sample_embedding = embeddings.embed_query(sample_text)\n",
    "print(f\"Embedding dimension: {len(sample_embedding)}\")\n",
    "print(f\"Sample embedding (first 5 values): {sample_embedding[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Store Embeddings in Pinecone\n",
    "\n",
    "We'll create a Pinecone index, store the document embeddings, and set up a LangChain vector store for retrieval."[](https://blog.gopenai.com/primer-on-vector-databases-and-retrieval-augmented-generation-rag-using-langchain-pinecone-37a27fb10546?gi=c3a16f2e233d)
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pinecone index\n",
    "index_name = 'rag-demo'\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    pinecone.create_index(index_name, dimension=384, metric='cosine')\n",
    "\n",
    "# Initialize Pinecone index\n",
    "index = pinecone.Index(index_name)\n",
    "\n",
    "# Store documents in Pinecone\n",
    "pinecone_store = Pinecone.from_documents(docs, embeddings, index_name=index_name)\n",
    "\n",
    "print(f\"Pinecone index '{index_name}' created and populated with {len(docs)} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Store Embeddings in ChromaDB\n",
    "\n",
    "We'll create a ChromaDB collection, store the same document embeddings, and set up a LangChain vector store."[](https://blog.futuresmart.ai/using-langchain-and-open-source-vector-db-chroma-for-semantic-search-with-openais-llm)
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ChromaDB client\n",
    "chroma_client = chromadb.PersistentClient(path='./chroma_db')\n",
    "\n",
    "# Store documents in ChromaDB\n",
    "chroma_store = Chroma.from_documents(\n",
    "    docs,\n",
    "    embeddings,\n",
    "    client=chroma_client,\n",
    "    collection_name='rag-demo',\n",
    "    persist_directory='./chroma_db'\n",
    ")\n",
    "\n",
    "print(f\"ChromaDB collection 'rag-demo' created and populated with {len(docs)} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Set Up RAG Pipeline\n",
    "\n",
    "We'll use LangChain's `RetrievalQA` chain with an Ollama-hosted LLM (`llama3`) to create RAG pipelines for both Pinecone and ChromaDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM (Ollama with llama3)\n",
    "llm = OllamaLLM(model='llama3')\n",
    "\n",
    "# Create RetrievalQA chains\n",
    "pinecone_qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type='stuff',\n",
    "    retriever=pinecone_store.as_retriever(search_kwargs={'k': 3})\n",
    ")\n",
    "\n",
    "chroma_qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type='stuff',\n",
    "    retriever=chroma_store.as_retriever(search_kwargs={'k': 3})\n",
    ")\n",
    "\n",
    "print(\"RAG pipelines initialized for Pinecone and ChromaDB.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Test RAG Pipelines\n",
    "\n",
    "We'll test both RAG pipelines with sample queries and measure response time and relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample queries\n",
    "queries = [\n",
    "    \"What is supervised learning?\",\n",
    "    \"What are some applications of machine learning?\",\n",
    "    \"What is reinforcement learning used for?\"\n",
    "]\n",
    "\n",
    "# Test and compare\n",
    "for query in queries:\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    \n",
    "    # Pinecone\n",
    "    start_time = time.time()\n",
    "    pinecone_result = pinecone_qa.run(query)\n",
    "    pinecone_time = time.time() - start_time\n",
    "    print(f\"Pinecone Response (Time: {pinecone_time:.2f}s):\")\n",
    "    print(pinecone_result)\n",
    "    \n",
    "    # ChromaDB\n",
    "    start_time = time.time()\n",
    "    chroma_result = chroma_qa.run(query)\n",
    "    chroma_time = time.time() - start_time\n",
    "    print(f\"ChromaDB Response (Time: {chroma_time:.2f}s):\")\n",
    "    print(chroma_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Comparison of Pinecone and ChromaDB\n",
    "\n",
    "### Pinecone\n",
    "- **Pros**:\n",
    "  - Fully managed, scalable vector database\n",
    "  - Optimized for real-time similarity search (cosine, Euclidean)\n",[](https://blog.gopenai.com/primer-on-vector-databases-and-retrieval-augmented-generation-rag-using-langchain-pinecone-37a27fb10546?gi=c3a16f2e233d)
    "  - Easy integration with LangChain\n",[](https://python.langchain.com/docs/integrations/vectorstores/pinecone/)
    "  - Suitable for production with high throughput\n",
    "- **Cons**:\n",
    "  - Requires an account and API key; free tier limited to 100,000 vectors\n",[](https://medium.com/%40sakhamurijaikar/which-vector-database-is-right-for-your-generative-ai-application-pinecone-vs-chromadb-1d849dd5e9df)
    "  - Costly for enterprise use\n",[](https://x.com/doesdatmaksense/status/1850505956821741757)
    "- **Performance**: Fast response times (~500ms for small indexes)\n",[](https://medium.com/%40sakhamurijaikar/which-vector-database-is-right-for-your-generative-ai-application-pinecone-vs-chromadb-1d849dd5e9df)
    "\n",
    "### ChromaDB\n",
    "- **Pros**:\n",
    "  - Open-source and free to use\n",[](https://blog.futuresmart.ai/using-langchain-and-open-source-vector-db-chroma-for-semantic-search-with-openais-llm)
    "  - Easy to set up locally or in Docker\n",[](https://dev.to/admantium/retrieval-augmented-generation-frameworks-langchain-iom)
    "  - Persists data locally (e.g., Parquet files)\n",[](https://developer.dataiku.com/latest/tutorials/machine-learning/genai/nlp/gpt-lc-chroma-rag/index.html)
    "  - Good for prototyping and development\n",[](https://medium.com/%40sakhamurijaikar/which-vector-database-is-right-for-your-generative-ai-application-pinecone-vs-chromadb-1d849dd5e9df)
    "- **Cons**:\n",
    "  - Self-hosted, requiring infrastructure management\n",[](https://medium.com/%40sakhamurijaikar/which-vector-database-is-right-for-your-generative-ai-application-pinecone-vs-chromadb-1d849dd5e9df)
    "  - Less optimized for large-scale production compared to Pinecone\n",
    "- **Performance**: Slightly slower than Pinecone for large datasets; no clear SLAs\n",[](https://medium.com/%40sakhamurijaikar/which-vector-database-is-right-for-your-generative-ai-application-pinecone-vs-chromadb-1d849dd5e9df)
    "\n",
    "## Recommendations\n",
    "- **Use Pinecone** for production-grade applications requiring scalability, real-time updates, and managed infrastructure.\n",
    "- **Use ChromaDB** for prototyping, local testing, or budget-constrained projects where self-hosting is feasible.\n",
    "\n",
    "## Notes\n",
    "- The example uses a small dataset for simplicity. For larger datasets, ensure proper chunking and indexing strategies.\n",
    "- Response times depend on network latency (Pinecone), hardware (ChromaDB), and LLM performance (Ollama).\n",
    "- For advanced RAG, consider hybrid search or reranking to improve relevance.\n",[](https://superlinked.com/vectorhub/articles/optimizing-rag-with-hybrid-search-reranking)
    "\n",
    "## Cleanup\n",
    "\n",
    "Delete the Pinecone index and ChromaDB collection to avoid resource usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup Pinecone\n",
    "if index_name in pinecone.list_indexes():\n",
    "    pinecone.delete_index(index_name)\n",
    "    print(f\"Deleted Pinecone index '{index_name}'.\")\n",
    "\n",
    "# Cleanup ChromaDB\n",
    "chroma_client.delete_collection('rag-demo')\n",
    "print(\"Deleted ChromaDB collection 'rag-demo'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation\n",
    "\n",
    "- **Document Loading and Splitting**: Used `TextLoader` and `CharacterTextSplitter` to load and chunk a sample text file about machine learning.\n",[](https://blog.gopenai.com/primer-on-vector-databases-and-retrieval-augmented-generation-rag-using-langchain-pinecone-37a27fb10546?gi=c3a16f2e233d)
    "- **Embeddings**: Employed Hugging Face's `all-MiniLM-L6-v2` for 384-dimensional embeddings, compatible with both vector databases.\n",[](https://battox.medium.com/retrieval-augmented-generation-in-very-few-lines-of-code-ba9ae02425eb)
    "- **Pinecone**: Created a managed index, stored embeddings, and set up a `Pinecone` vector store for retrieval.\n",[](https://python.langchain.com/docs/integrations/vectorstores/pinecone/)
    "- **ChromaDB**: Created a local collection, stored embeddings, and set up a `Chroma` vector store.\n",[](https://python.langchain.com/docs/integrations/vectorstores/chroma/)
    "- **RAG Pipeline**: Used `RetrievalQA` with an Ollama-hosted `llama3` LLM to answer queries, retrieving top-3 relevant chunks.\n",[](https://propulsionhq.com/blog/rag-llama2-langchain-chromadb-on-propulsion-ai)
    "- **Comparison**: Evaluated Pinecone and ChromaDB based on setup, performance, and use cases, highlighting Pinecone's scalability and ChromaDB's open-source flexibility.\n",[](https://medium.com/%40sakhamurijaikar/which-vector-database-is-right-for-your-generative-ai-application-pinecone-vs-chromadb-1d849dd5e9df)
    "\n",
    "To extend this work, consider:\n",
    "- Using larger datasets (e.g., PDFs, web pages) with advanced loaders\n",[](https://developer.dataiku.com/12/tutorials/machine-learning/genai/nlp/gpt-lc-chroma-rag/index.html)
    "- Implementing hybrid search for improved retrieval\n",[](https://superlinked.com/vectorhub/articles/optimizing-rag-with-hybrid-search-reranking)
    "- Fine-tuning the LLM for domain-specific tasks\n",[](https://propulsionhq.com/blog/rag-llama2-langchain-chromadb-on-propulsion-ai)
    "- Adding prompt templates and output parsers for structured responses\n",[](https://developer.dataiku.com/latest/tutorials/machine-learning/genai/nlp/gpt-lc-chroma-rag/index.html)
    "- Deploying the RAG system with a UI using Chainlit"[](https://medium.com/%40sudarshan-koirala/building-a-powerful-vector-search-engine-with-pinecone-chromadb-and-faiss-using-langchain-c2086a876a91)
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}