{"cells":[{"cell_type":"markdown","metadata":{"id":"NeKi0XMlHKXf"},"source":["# Lecture 71: Sequence Models in NLP\n","\n","This notebook introduces **sequence models** in Natural Language Processing (NLP), focusing on **Recurrent Neural Networks (RNNs)** and **Long Short-Term Memory (LSTM)** networks. We'll use these models for a binary text classification task (sentiment analysis) on the IMDb movie review dataset. The notebook covers:\n","\n","- Understanding RNNs and LSTMs for sequential data\n","- Loading and preprocessing the IMDb dataset\n","- Building and training a simple RNN model\n","- Building and training an LSTM model\n","- Comparing model performance\n","- Visualizing training results\n","\n","RNNs process sequential data by maintaining a hidden state, while LSTMs address vanishing gradient issues, making them better suited for long sequences."]},{"cell_type":"markdown","metadata":{"id":"Kf68Sk_8HKX2"},"source":["## Setup and Imports\n","\n","Let's import the necessary libraries and set up the environment for reproducibility."]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":7163,"status":"ok","timestamp":1749229652033,"user":{"displayName":"Abdullahi Ahmad","userId":"03775884294431603545"},"user_tz":-60},"id":"3Nc7MquZHKYC"},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.datasets import imdb\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, SimpleRNN, LSTM, Dense\n","import matplotlib.pyplot as plt\n","\n","# Set random seed for reproducibility\n","np.random.seed(42)\n","tf.random.set_seed(42)"]},{"cell_type":"markdown","metadata":{"id":"-8tO4tSdHKYH"},"source":["## Understanding Sequence Models\n","\n","- **RNNs (Recurrent Neural Networks)**:\n","  - Process sequences by maintaining a hidden state passed from one time step to the next.\n","  - Pros: Suitable for sequential data like text or time series.\n","  - Cons: Suffers from vanishing/exploding gradients, struggles with long-term dependencies.\n","- **LSTMs (Long Short-Term Memory)**:\n","  - An advanced RNN variant with memory cells and gates (input, forget, output).\n","  - Pros: Captures long-term dependencies, mitigates vanishing gradient problem.\n","  - Cons: More computationally expensive than simple RNNs.\n","\n","We'll use both for sentiment analysis to compare their performance."]},{"cell_type":"markdown","metadata":{"id":"CS19QyXnHKYI"},"source":["## Loading and Preprocessing the IMDb Dataset\n","\n","The IMDb dataset contains 50,000 movie reviews (25,000 train, 25,000 test) labeled as positive (1) or negative (0). We'll preprocess the data by limiting the vocabulary size, padding sequences to a fixed length, and preparing it for the models."]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10711,"status":"ok","timestamp":1749229662749,"user":{"displayName":"Abdullahi Ahmad","userId":"03775884294431603545"},"user_tz":-60},"id":"Ebs0EUgZHKYU","outputId":"4b14d1f6-8620-4992-cb9d-f7f29cf264bd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n","\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n","Training data shape: (25000, 200)\n","Test data shape: (25000, 200)\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n","\u001b[1m1641221/1641221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1us/step\n","\n","Sample Decoded Review:\n","and you could just imagine being there robert ? is an amazing actor and now the same being director ...\n"]}],"source":["# Parameters\n","max_features = 10000  # Number of words to consider (top 10,000 most frequent)\n","maxlen = 200         # Maximum sequence length\n","\n","# Load IMDb dataset\n","(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features)\n","\n","# Pad sequences to ensure uniform length\n","X_train = pad_sequences(X_train, maxlen=maxlen)\n","X_test = pad_sequences(X_test, maxlen=maxlen)\n","\n","print(f\"Training data shape: {X_train.shape}\")\n","print(f\"Test data shape: {X_test.shape}\")\n","\n","# Example of a preprocessed review\n","word_index = imdb.get_word_index()\n","reverse_word_index = {value: key for key, value in word_index.items()}\n","decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in X_train[0]])\n","print(\"\\nSample Decoded Review:\")\n","print(decoded_review[:100] + \"...\")"]},{"cell_type":"markdown","metadata":{"id":"AL8A7ZzHHKYV"},"source":["## Building and Training a Simple RNN Model\n","\n","We'll create a simple RNN model with an embedding layer to convert words into dense vectors, a SimpleRNN layer for sequence processing, and a dense output layer for binary classification."]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":446},"executionInfo":{"elapsed":27401,"status":"ok","timestamp":1749229695165,"user":{"displayName":"Abdullahi Ahmad","userId":"03775884294431603545"},"user_tz":-60},"id":"R08kAdzdHKYX","outputId":"61d31206-49ab-440f-a1bf-34f0e0a420b6"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n","  warnings.warn(\n"]},{"data":{"text/html":["\u003cpre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"\u003e\u003cspan style=\"font-weight: bold\"\u003eModel: \"sequential\"\u003c/span\u003e\n","\u003c/pre\u003e\n"],"text/plain":["\u001b[1mModel: \"sequential\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\u003cpre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"\u003e┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u003cspan style=\"font-weight: bold\"\u003e Layer (type)                    \u003c/span\u003e┃\u003cspan style=\"font-weight: bold\"\u003e Output Shape           \u003c/span\u003e┃\u003cspan style=\"font-weight: bold\"\u003e       Param # \u003c/span\u003e┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ embedding (\u003cspan style=\"color: #0087ff; text-decoration-color: #0087ff\"\u003eEmbedding\u003c/span\u003e)           │ ?                      │   \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e0\u003c/span\u003e (unbuilt) │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ simple_rnn (\u003cspan style=\"color: #0087ff; text-decoration-color: #0087ff\"\u003eSimpleRNN\u003c/span\u003e)          │ ?                      │   \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e0\u003c/span\u003e (unbuilt) │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense (\u003cspan style=\"color: #0087ff; text-decoration-color: #0087ff\"\u003eDense\u003c/span\u003e)                   │ ?                      │   \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e0\u003c/span\u003e (unbuilt) │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n","\u003c/pre\u003e\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ simple_rnn (\u001b[38;5;33mSimpleRNN\u001b[0m)          │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\u003cpre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"\u003e\u003cspan style=\"font-weight: bold\"\u003e Total params: \u003c/span\u003e\u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e0\u003c/span\u003e (0.00 B)\n","\u003c/pre\u003e\n"],"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\u003cpre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"\u003e\u003cspan style=\"font-weight: bold\"\u003e Trainable params: \u003c/span\u003e\u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e0\u003c/span\u003e (0.00 B)\n","\u003c/pre\u003e\n"],"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\u003cpre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"\u003e\u003cspan style=\"font-weight: bold\"\u003e Non-trainable params: \u003c/span\u003e\u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e0\u003c/span\u003e (0.00 B)\n","\u003c/pre\u003e\n"],"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 32ms/step - accuracy: 0.5784 - loss: 0.6555 - val_accuracy: 0.7580 - val_loss: 0.5052\n","Epoch 2/5\n","\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - accuracy: 0.8311 - loss: 0.3849 - val_accuracy: 0.8302 - val_loss: 0.4064\n","Epoch 3/5\n","\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - accuracy: 0.9208 - loss: 0.2050 - val_accuracy: 0.7432 - val_loss: 0.5806\n","Epoch 4/5\n","\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - accuracy: 0.9554 - loss: 0.1331 - val_accuracy: 0.7934 - val_loss: 0.5827\n","Epoch 5/5\n","\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - accuracy: 0.9833 - loss: 0.0572 - val_accuracy: 0.7826 - val_loss: 0.6885\n"]}],"source":["# Build RNN model\n","rnn_model = Sequential([\n","    Embedding(max_features, 128, input_length=maxlen),\n","    SimpleRNN(64, return_sequences=False),\n","    Dense(1, activation='sigmoid')\n","])\n","\n","# Compile the model\n","rnn_model.compile(optimizer='adam',\n","                  loss='binary_crossentropy',\n","                  metrics=['accuracy'])\n","\n","# Model summary\n","rnn_model.summary()\n","\n","# Train the model\n","rnn_history = rnn_model.fit(X_train, y_train,\n","                            epochs=5,\n","                            batch_size=128,\n","                            validation_split=0.2,\n","                            verbose=1)"]},{"cell_type":"markdown","metadata":{"id":"PUycr5dxHKYZ"},"source":["## Building and Training an LSTM Model\n","\n","Next, we'll create an LSTM model with a similar architecture but using an LSTM layer instead of SimpleRNN to better handle long-term dependencies."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":267},"id":"DjjxHYGEHKYa"},"outputs":[{"data":{"text/html":["\u003cpre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"\u003e\u003cspan style=\"font-weight: bold\"\u003eModel: \"sequential_1\"\u003c/span\u003e\n","\u003c/pre\u003e\n"],"text/plain":["\u001b[1mModel: \"sequential_1\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\u003cpre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"\u003e┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u003cspan style=\"font-weight: bold\"\u003e Layer (type)                    \u003c/span\u003e┃\u003cspan style=\"font-weight: bold\"\u003e Output Shape           \u003c/span\u003e┃\u003cspan style=\"font-weight: bold\"\u003e       Param # \u003c/span\u003e┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ embedding_1 (\u003cspan style=\"color: #0087ff; text-decoration-color: #0087ff\"\u003eEmbedding\u003c/span\u003e)         │ ?                      │   \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e0\u003c/span\u003e (unbuilt) │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ lstm (\u003cspan style=\"color: #0087ff; text-decoration-color: #0087ff\"\u003eLSTM\u003c/span\u003e)                     │ ?                      │   \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e0\u003c/span\u003e (unbuilt) │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_1 (\u003cspan style=\"color: #0087ff; text-decoration-color: #0087ff\"\u003eDense\u003c/span\u003e)                 │ ?                      │   \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e0\u003c/span\u003e (unbuilt) │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n","\u003c/pre\u003e\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\u003cpre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"\u003e\u003cspan style=\"font-weight: bold\"\u003e Total params: \u003c/span\u003e\u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e0\u003c/span\u003e (0.00 B)\n","\u003c/pre\u003e\n"],"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\u003cpre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"\u003e\u003cspan style=\"font-weight: bold\"\u003e Trainable params: \u003c/span\u003e\u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e0\u003c/span\u003e (0.00 B)\n","\u003c/pre\u003e\n"],"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\u003cpre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"\u003e\u003cspan style=\"font-weight: bold\"\u003e Non-trainable params: \u003c/span\u003e\u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e0\u003c/span\u003e (0.00 B)\n","\u003c/pre\u003e\n"],"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 16ms/step - accuracy: 0.6978 - loss: 0.5464 - val_accuracy: 0.8532 - val_loss: 0.3437\n","Epoch 2/5\n","\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.8968 - loss: 0.2623 - val_accuracy: 0.8532 - val_loss: 0.3595\n","Epoch 3/5\n","\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.9217 - loss: 0.2065 - val_accuracy: 0.8470 - val_loss: 0.4393\n","Epoch 4/5\n","\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9248 - loss: 0.1963 - val_accuracy: 0.8322 - val_loss: 0.3826\n","Epoch 5/5\n","\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.9461 - loss: 0.1508 - val_accuracy: 0.8614 - val_loss: 0.3753\n"]}],"source":["# Build LSTM model\n","lstm_model = Sequential([\n","    Embedding(max_features, 128, input_length=maxlen),\n","    LSTM(64, return_sequences=False),\n","    Dense(1, activation='sigmoid')\n","])\n","\n","# Compile the model\n","lstm_model.compile(optimizer='adam',\n","                   loss='binary_crossentropy',\n","                   metrics=['accuracy'])\n","\n","# Model summary\n","lstm_model.summary()\n","\n","# Train the model\n","lstm_history = lstm_model.fit(X_train, y_train,\n","                              epochs=5,\n","                              batch_size=128,\n","                              validation_split=0.2,\n","                              verbose=1)"]},{"cell_type":"markdown","metadata":{"id":"qxoroDHvHKYm"},"source":["## Evaluating and Comparing Model Performance\n","\n","We'll evaluate both models on the test set and visualize their training and validation accuracy/loss to compare performance."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Zk60HPwHKYm"},"outputs":[],"source":["# Evaluate RNN model\n","rnn_test_loss, rnn_test_accuracy = rnn_model.evaluate(X_test, y_test, verbose=0)\n","print(f\"RNN Test Accuracy: {rnn_test_accuracy:.4f}\")\n","print(f\"RNN Test Loss: {rnn_test_loss:.4f}\")\n","\n","# Evaluate LSTM model\n","lstm_test_loss, lstm_test_accuracy = lstm_model.evaluate(X_test, y_test, verbose=0)\n","print(f\"LSTM Test Accuracy: {lstm_test_accuracy:.4f}\")\n","print(f\"LSTM Test Loss: {lstm_test_loss:.4f}\")\n","\n","# Plot training history\n","plt.figure(figsize=(12, 8))\n","\n","# Plot accuracy\n","plt.subplot(2, 2, 1)\n","plt.plot(rnn_history.history['accuracy'], label='RNN Training Accuracy')\n","plt.plot(rnn_history.history['val_accuracy'], label='RNN Validation Accuracy')\n","plt.title('RNN Model Accuracy')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","\n","plt.subplot(2, 2, 2)\n","plt.plot(lstm_history.history['accuracy'], label='LSTM Training Accuracy')\n","plt.plot(lstm_history.history['val_accuracy'], label='LSTM Validation Accuracy')\n","plt.title('LSTM Model Accuracy')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","\n","# Plot loss\n","plt.subplot(2, 2, 3)\n","plt.plot(rnn_history.history['loss'], label='RNN Training Loss')\n","plt.plot(rnn_history.history['val_loss'], label='RNN Validation Loss')\n","plt.title('RNN Model Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","plt.subplot(2, 2, 4)\n","plt.plot(lstm_history.history['loss'], label='LSTM Training Loss')\n","plt.plot(lstm_history.history['val_loss'], label='LSTM Validation Loss')\n","plt.title('LSTM Model Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","plt.tight_layout()\n","plt.savefig('sequence_models_history.png')"]},{"cell_type":"markdown","metadata":{"id":"BoViMwj_HKYo"},"source":["## Making Predictions\n","\n","Let's make predictions on a few test reviews to see how the LSTM model (likely the better performer) classifies sentiment."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QGB4kWBKHKYp"},"outputs":[],"source":["# Select a few test reviews\n","num_samples = 5\n","sample_indices = np.random.choice(X_test.shape[0], num_samples, replace=False)\n","sample_reviews = X_test[sample_indices]\n","sample_labels = y_test[sample_indices]\n","\n","# Predict with LSTM model\n","predictions = lstm_model.predict(sample_reviews)\n","predicted_classes = (predictions \u003e 0.5).astype(int).flatten()\n","\n","# Decode reviews for readability\n","print(\"\\nSample Predictions (LSTM Model):\")\n","for i, idx in enumerate(sample_indices):\n","    decoded_review = ' '.join([reverse_word_index.get(word - 3, '?') for word in sample_reviews[i]])\n","    print(f\"\\nReview {i+1}: {decoded_review[:100]}...\")\n","    print(f\"True Sentiment: {'Positive' if sample_labels[i] == 1 else 'Negative'}\")\n","    print(f\"Predicted Sentiment: {'Positive' if predicted_classes[i] == 1 else 'Negative'}\")\n","    print(f\"Prediction Probability: {predictions[i][0]:.4f}\")"]},{"cell_type":"markdown","metadata":{"id":"Y8AmAufiHKYq"},"source":["## Explanation\n","\n","- **Sequence Models**:\n","  - **RNNs**: Process sequences but struggle with long-term dependencies due to vanishing gradients.\n","  - **LSTMs**: Use memory cells and gates to retain long-term information, making them more effective for NLP tasks.\n","- **Dataset**: IMDb dataset with 50,000 reviews, preprocessed to limit vocabulary and pad sequences.\n","- **Models**:\n","  - RNN: Simple architecture with embedding and SimpleRNN layers.\n","  - LSTM: Similar architecture but with an LSTM layer for better sequence modeling.\n","- **Training**: Both models trained for 5 epochs with Adam optimizer and binary crossentropy loss.\n","- **Evaluation**: Compared test accuracy and loss, visualized training history to assess overfitting.\n","- **Predictions**: Demonstrated LSTM predictions on sample reviews, showing practical application.\n","\n","To extend this work, consider:\n","- Using bidirectional LSTMs or GRUs for improved performance\n","- Adding dropout or regularization to prevent overfitting\n","- Incorporating pre-trained embeddings (e.g., GloVe, Word2Vec)\n","- Exploring attention mechanisms or transformers for advanced sequence modeling"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"}},"nbformat":4,"nbformat_minor":0}