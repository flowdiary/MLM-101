{"cells":[{"cell_type":"markdown","metadata":{"id":"ngw4ea2PvDzI"},"source":["# Lecture 67: Introduction to NLP\n","\n","This notebook introduces **Natural Language Processing (NLP)** with a focus on fundamental **text preprocessing** techniques: **tokenization**, **stemming**, and **lemmatization**. These steps are essential for preparing text data for NLP tasks such as sentiment analysis, text classification, and machine translation. The notebook covers:\n","\n","- Overview of NLP and preprocessing\n","- Tokenization: Splitting text into words or sentences\n","- Stemming: Reducing words to their root form\n","- Lemmatization: Converting words to their dictionary form\n","- Practical implementation using NLTK\n","- Visualizing preprocessing results\n","\n","We'll use a small sample dataset to demonstrate these techniques."]},{"cell_type":"markdown","metadata":{"id":"MgMmmUPrvDzO"},"source":["## Setup and Imports\n","\n","Let's import the necessary libraries and download required NLTK data. We'll use NLTK for tokenization, stemming, and lemmatization."]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h_sbklTtvDzP","executionInfo":{"status":"ok","timestamp":1749223904823,"user_tz":-60,"elapsed":7,"user":{"displayName":"Abdullahi Ahmad","userId":"03775884294431603545"}},"outputId":"57319ea1-1e89-469a-8777-a31faef7abd1"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n","[nltk_data]       date!\n"]}],"source":["import nltk\n","import re\n","import pandas as pd\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","from nltk.stem import PorterStemmer, WordNetLemmatizer\n","from nltk.corpus import wordnet\n","\n","# Download required NLTK data\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('punkt_tab')\n","nltk.download('averaged_perceptron_tagger_eng')\n","# Set random seed for reproducibility (if needed)\n","import numpy as np\n","np.random.seed(42)"]},{"cell_type":"markdown","metadata":{"id":"6pglL2OcvDzR"},"source":["## Overview of Text Preprocessing\n","\n","Text preprocessing transforms raw text into a format suitable for NLP tasks. Key techniques include:\n","\n","- **Tokenization**: Splitting text into smaller units (e.g., words or sentences).\n","- **Stemming**: Reducing words to their root or base form by removing suffixes (e.g., 'running' → 'run').\n","- **Lemmatization**: Converting words to their dictionary form, considering context (e.g., 'better' → 'good').\n","\n","These steps help standardize text, reduce vocabulary size, and improve model performance."]},{"cell_type":"markdown","metadata":{"id":"0QpYzzLXvDzS"},"source":["## Sample Dataset\n","\n","We'll use a small set of sample sentences to demonstrate preprocessing techniques. These sentences are diverse to highlight the effects of tokenization, stemming, and lemmatization."]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fI0pMsF2vDzS","executionInfo":{"status":"ok","timestamp":1749223975385,"user_tz":-60,"elapsed":56,"user":{"displayName":"Abdullahi Ahmad","userId":"03775884294431603545"}},"outputId":"ca7b0a9d-9b45-4d8e-a36d-50c19c97abd0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sample Texts:\n","Text 1: The cats are running quickly in the gardens!\n","Text 2: She was happily studying complex algorithms.\n","Text 3: Better solutions improve our lives daily.\n","Text 4: They have been working on innovative projects.\n"]}],"source":["# Sample texts\n","texts = [\n","    \"The cats are running quickly in the gardens!\",\n","    \"She was happily studying complex algorithms.\",\n","    \"Better solutions improve our lives daily.\",\n","    \"They have been working on innovative projects.\"\n","]\n","\n","# Display sample texts\n","print(\"Sample Texts:\")\n","for i, text in enumerate(texts, 1):\n","    print(f\"Text {i}: {text}\")"]},{"cell_type":"markdown","metadata":{"id":"K9L7eABavDzT"},"source":["## Text Preprocessing Function\n","\n","We'll create a function to preprocess text, including cleaning, tokenization, stemming, and lemmatization. For lemmatization, we'll use part-of-speech (POS) tagging to ensure accurate results."]},{"cell_type":"code","execution_count":13,"metadata":{"id":"vbIzvyL8vDzU","executionInfo":{"status":"ok","timestamp":1749224030487,"user_tz":-60,"elapsed":802,"user":{"displayName":"Abdullahi Ahmad","userId":"03775884294431603545"}}},"outputs":[],"source":["def get_wordnet_pos(word):\n","    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n","    tag = nltk.pos_tag([word])[0][1][0].upper()\n","    tag_dict = {\n","        'J': wordnet.ADJ,\n","        'N': wordnet.NOUN,\n","        'V': wordnet.VERB,\n","        'R': wordnet.ADV\n","    }\n","    return tag_dict.get(tag, wordnet.NOUN)\n","\n","def preprocess_text(text):\n","    \"\"\"Perform text preprocessing: cleaning, tokenization, stemming, and lemmatization\"\"\"\n","    # 1. Text Cleaning\n","    text = text.lower()\n","    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n","\n","    # 2. Tokenization\n","    sentences = sent_tokenize(text)\n","    tokens = word_tokenize(text)\n","\n","    # 3. Stemming\n","    stemmer = PorterStemmer()\n","    stemmed_words = [stemmer.stem(token) for token in tokens]\n","\n","    # 4. Lemmatization\n","    lemmatizer = WordNetLemmatizer()\n","    lemmatized_words = [lemmatizer.lemmatize(token, get_wordnet_pos(token)) for token in tokens]\n","\n","    return {\n","        'original': text,\n","        'sentences': sentences,\n","        'tokens': tokens,\n","        'stemmed': stemmed_words,\n","        'lemmatized': lemmatized_words\n","    }"]},{"cell_type":"markdown","metadata":{"id":"Yi2xBp9pvDzV"},"source":["## Applying Preprocessing\n","\n","We'll apply the preprocessing function to each sample text and store the results for analysis."]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vD73GZVvvDzV","executionInfo":{"status":"ok","timestamp":1749224039001,"user_tz":-60,"elapsed":211,"user":{"displayName":"Abdullahi Ahmad","userId":"03775884294431603545"}},"outputId":"1ebc1d2a-f714-4e2f-a764-230a28e3cfe8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Preprocessing Results:\n","\n","Text 1:\n","Original: the cats are running quickly in the gardens\n","Sentences: ['the cats are running quickly in the gardens']\n","Tokens: ['the', 'cats', 'are', 'running', 'quickly', 'in', 'the', 'gardens']\n","Stemmed: ['the', 'cat', 'are', 'run', 'quickli', 'in', 'the', 'garden']\n","Lemmatized: ['the', 'cat', 'be', 'run', 'quickly', 'in', 'the', 'garden']\n","\n","Text 2:\n","Original: she was happily studying complex algorithms\n","Sentences: ['she was happily studying complex algorithms']\n","Tokens: ['she', 'was', 'happily', 'studying', 'complex', 'algorithms']\n","Stemmed: ['she', 'wa', 'happili', 'studi', 'complex', 'algorithm']\n","Lemmatized: ['she', 'be', 'happily', 'study', 'complex', 'algorithm']\n","\n","Text 3:\n","Original: better solutions improve our lives daily\n","Sentences: ['better solutions improve our lives daily']\n","Tokens: ['better', 'solutions', 'improve', 'our', 'lives', 'daily']\n","Stemmed: ['better', 'solut', 'improv', 'our', 'live', 'daili']\n","Lemmatized: ['well', 'solution', 'improve', 'our', 'life', 'daily']\n","\n","Text 4:\n","Original: they have been working on innovative projects\n","Sentences: ['they have been working on innovative projects']\n","Tokens: ['they', 'have', 'been', 'working', 'on', 'innovative', 'projects']\n","Stemmed: ['they', 'have', 'been', 'work', 'on', 'innov', 'project']\n","Lemmatized: ['they', 'have', 'be', 'work', 'on', 'innovative', 'project']\n"]}],"source":["# Process all texts\n","results = [preprocess_text(text) for text in texts]\n","\n","# Display results\n","print(\"Preprocessing Results:\")\n","for i, result in enumerate(results, 1):\n","    print(f\"\\nText {i}:\")\n","    print(f\"Original: {result['original']}\")\n","    print(f\"Sentences: {result['sentences']}\")\n","    print(f\"Tokens: {result['tokens']}\")\n","    print(f\"Stemmed: {result['stemmed']}\")\n","    print(f\"Lemmatized: {result['lemmatized']}\")"]},{"cell_type":"markdown","metadata":{"id":"SUt9dmKtvDzW"},"source":["## Visualizing Preprocessing Results\n","\n","To better understand the differences, we'll create a table comparing the original tokens, stemmed words, and lemmatized words for one of the texts."]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pNLe6QtLvDzW","executionInfo":{"status":"ok","timestamp":1749224127415,"user_tz":-60,"elapsed":7,"user":{"displayName":"Abdullahi Ahmad","userId":"03775884294431603545"}},"outputId":"dd822782-fa0b-4809-a155-d389fb79316d"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Comparison for Text 1: The cats are running quickly in the gardens!\n","     Token  Stemmed Lemmatized\n","0      the      the        the\n","1     cats      cat        cat\n","2      are      are         be\n","3  running      run        run\n","4  quickly  quickli    quickly\n","5       in       in         in\n","6      the      the        the\n","7  gardens   garden     garden\n"]}],"source":["# Select the first text for visualization\n","sample_result = results[0]\n","\n","# Create a DataFrame for comparison\n","comparison_df = pd.DataFrame({\n","    'Token': sample_result['tokens'],\n","    'Stemmed': sample_result['stemmed'],\n","    'Lemmatized': sample_result['lemmatized']\n","})\n","\n","print(f\"\\nComparison for Text 1: {texts[0]}\")\n","print(comparison_df)\n","\n","# Save the comparison table to a file\n","comparison_df.to_csv('preprocessing_comparison.csv', index=False)"]},{"cell_type":"markdown","metadata":{"id":"Q-mktfe3vDzX"},"source":["## Explanation\n","\n","- **Tokenization**:\n","  - Splits text into sentences or words (e.g., 'cats' and 'running' as tokens).\n","  - Essential for breaking down text into manageable units.\n","- **Stemming**:\n","  - Reduces words to their root form (e.g., 'running' → 'run', 'gardens' → 'garden').\n","  - Fast but may produce non-words (e.g., 'better' → 'better').\n","- **Lemmatization**:\n","  - Converts words to their dictionary form using context (e.g., 'running' → 'run', 'better' → 'good').\n","  - More accurate but computationally intensive.\n","- **Implementation**:\n","  - Used NLTK's `word_tokenize`, `PorterStemmer`, and `WordNetLemmatizer` with POS tagging for accurate lemmatization.\n","  - Applied cleaning to remove special characters and convert to lowercase.\n","- **Visualization**:\n","  - Created a table to compare tokens, stemmed, and lemmatized words, highlighting differences.\n","\n","To extend this work, consider:\n","- Adding stop word removal for further preprocessing\n","- Using other libraries like spaCy for advanced tokenization and lemmatization\n","- Applying these techniques to a larger dataset for downstream tasks (e.g., text classification)\n","- Exploring other stemming algorithms (e.g., SnowballStemmer)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}