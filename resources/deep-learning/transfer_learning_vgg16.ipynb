{"cells":[{"cell_type":"markdown","metadata":{"id":"k6imB_QARRdi"},"source":["# Lecture 66: Transfer Learning in Deep Learning\n","\n","This notebook demonstrates **transfer learning** using a pre-trained VGG16 model for image classification on the CIFAR-10 dataset. Transfer learning involves using a model pre-trained on a large dataset (e.g., ImageNet) and adapting it for a new task. We'll cover:\n","\n","- Loading and preprocessing the CIFAR-10 dataset\n","- Using a pre-trained VGG16 model (excluding top layers)\n","- Adding custom layers for the new task\n","- Freezing and fine-tuning the model\n","- Training and evaluating the model\n","- Visualizing predictions\n","\n","VGG16, pre-trained on ImageNet, will be used as the base model, with fine-tuning to adapt it to CIFAR-10's 10 classes."]},{"cell_type":"markdown","metadata":{"id":"DsbruCiIRRdk"},"source":["## Setup and Imports\n","\n","Let's import the necessary libraries and set up the environment for reproducibility."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"QT2kJqCyRRdk","executionInfo":{"status":"ok","timestamp":1749228146296,"user_tz":-60,"elapsed":11151,"user":{"displayName":"Abdullahi Ahmad","userId":"03775884294431603545"}}},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.applications import VGG16\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n","from tensorflow.keras.datasets import cifar10\n","import matplotlib.pyplot as plt\n","\n","# Set random seed for reproducibility\n","np.random.seed(42)\n","tf.random.set_seed(42)"]},{"cell_type":"markdown","metadata":{"id":"2i0a_lu1RRdq"},"source":["## Loading and Preprocessing the CIFAR-10 Dataset\n","\n","CIFAR-10 contains 60,000 32x32 RGB images across 10 classes. We'll preprocess the images to match VGG16's expected input format (resizing to 224x224 and applying VGG16-specific preprocessing) and convert labels to one-hot encoding."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BQKCmNjTRRdq","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8a5cdba7-61a1-4e3b-8f73-e516d259438d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n","\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 0us/step\n"]}],"source":["# Load CIFAR-10 dataset\n","(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n","\n","# Class names for CIFAR-10\n","class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n","               'dog', 'frog', 'horse', 'ship', 'truck']\n","\n","# Resize images to 224x224 for VGG16\n","def resize_images(images, target_size=(224, 224)):\n","    resized_images = np.zeros((images.shape[0], *target_size, 3))\n","    for i in range(images.shape[0]):\n","        resized_images[i] = tf.image.resize(images[i], target_size).numpy()\n","    return resized_images\n","\n","X_train = resize_images(X_train)\n","X_test = resize_images(X_test)\n","\n","# Normalize pixel values using VGG16 preprocessing\n","X_train = keras.applications.vgg16.preprocess_input(X_train)\n","X_test = keras.applications.vgg16.preprocess_input(X_test)\n","\n","# Convert labels to one-hot encoding\n","y_train = keras.utils.to_categorical(y_train, 10)\n","y_test = keras.utils.to_categorical(y_test, 10)\n","\n","print(f\"Training data shape: {X_train.shape}\")\n","print(f\"Test data shape: {X_test.shape}\")\n","\n","# Visualize some sample images\n","plt.figure(figsize=(10, 2))\n","for i in range(5):\n","    plt.subplot(1, 5, i+1)\n","    # Undo VGG16 preprocessing for visualization\n","    img = X_train[i].copy()\n","    img[:, :, 0] += 103.939\n","    img[:, :, 1] += 116.779\n","    img[:, :, 2] += 123.68\n","    img = img[:, :, ::-1]  # BGR to RGB\n","    img = np.clip(img, 0, 255).astype('uint8')\n","    plt.imshow(img)\n","    plt.title(class_names[np.argmax(y_train[i])])\n","    plt.axis('off')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"psuWw94hRRdr"},"source":["## Loading the Pre-trained VGG16 Model\n","\n","We'll load VGG16 pre-trained on ImageNet, excluding its top (fully connected) layers, and add custom layers for CIFAR-10 classification. We'll initially freeze the pre-trained layers to use their learned features."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":403},"id":"V0az5CB1RRds","executionInfo":{"status":"ok","timestamp":1746311787245,"user_tz":-60,"elapsed":7063,"user":{"displayName":"Abdullahi Ahmad","userId":"03775884294431603545"}},"outputId":"e8bdfd86-1762-408a-feab-ddb9c85c4e5a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n","\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"functional\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ vgg16 (\u001b[38;5;33mFunctional\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │    \u001b[38;5;34m14,714,688\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n","│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m2,570\u001b[0m │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ vgg16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,570</span> │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m14,848,586\u001b[0m (56.64 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,848,586</span> (56.64 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m133,898\u001b[0m (523.04 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">133,898</span> (523.04 KB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m14,714,688\u001b[0m (56.13 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> (56.13 MB)\n","</pre>\n"]},"metadata":{}}],"source":["# Load VGG16 without top layers\n","base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n","\n","# Freeze the base model layers\n","base_model.trainable = False\n","\n","# Add custom layers\n","inputs = keras.Input(shape=(224, 224, 3))\n","x = base_model(inputs, training=False)\n","x = GlobalAveragePooling2D()(x)\n","x = Dense(256, activation='relu')(x)\n","x = Dropout(0.5)(x)\n","outputs = Dense(10, activation='softmax')(x)\n","\n","# Create the model\n","model = Model(inputs, outputs)\n","\n","# Compile the model\n","model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),\n","              loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","model.summary()"]},{"cell_type":"markdown","metadata":{"id":"7zwGGaSLRRdt"},"source":["## Training the Model (Feature Extraction)\n","\n","First, we'll train the model with the pre-trained layers frozen, only updating the custom layers. This is known as **feature extraction**."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":216},"id":"1VrFEhzWRRdt","executionInfo":{"status":"error","timestamp":1746311799633,"user_tz":-60,"elapsed":454,"user":{"displayName":"Abdullahi Ahmad","userId":"03775884294431603545"}},"outputId":"ca78938e-0a3d-4700-a51a-0c8bd7eb8ac5"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'X_train' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-0216d19b17f3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Train the model (feature extraction)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m history = model.fit(X_train, y_train,\n\u001b[0m\u001b[1;32m     10\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"]}],"source":["# Define early stopping\n","early_stopping = keras.callbacks.EarlyStopping(\n","    monitor='val_loss',\n","    patience=5,\n","    restore_best_weights=True\n",")\n","\n","# Train the model (feature extraction)\n","history = model.fit(X_train, y_train,\n","                    epochs=20,\n","                    batch_size=32,\n","                    validation_split=0.2,\n","                    callbacks=[early_stopping],\n","                    verbose=1)"]},{"cell_type":"markdown","metadata":{"id":"Zl9GyXAARRdu"},"source":["## Fine-Tuning the Model\n","\n","To improve performance, we'll unfreeze some of the later layers of VGG16 and fine-tune them with a lower learning rate. This allows the model to adapt the pre-trained features to the CIFAR-10 dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ej-vkw3TRRdu"},"outputs":[],"source":["# Unfreeze the base model\n","base_model.trainable = True\n","\n","# Freeze all layers except the last convolutional block (block5)\n","for layer in base_model.layers:\n","    if 'block5' in layer.name:\n","        layer.trainable = True\n","    else:\n","        layer.trainable = False\n","\n","# Recompile the model with a lower learning rate\n","model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-5),\n","              loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","# Fine-tune the model\n","fine_tune_history = model.fit(X_train, y_train,\n","                              epochs=10,\n","                              batch_size=32,\n","                              validation_split=0.2,\n","                              callbacks=[early_stopping],\n","                              verbose=1)"]},{"cell_type":"markdown","metadata":{"id":"4sNhNE0ZRRdu"},"source":["## Evaluating the Model\n","\n","We'll evaluate the fine-tuned model on the test set and visualize the training and validation accuracy/loss curves."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d30tLKwGRRdv"},"outputs":[],"source":["# Evaluate on test set\n","test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n","print(f\"Test accuracy: {test_accuracy:.4f}\")\n","print(f\"Test loss: {test_loss:.4f}\")\n","\n","# Plot training history (combine feature extraction and fine-tuning)\n","plt.figure(figsize=(12, 4))\n","\n","# Plot accuracy\n","plt.subplot(1, 2, 1)\n","plt.plot(history.history['accuracy'] + fine_tune_history.history['accuracy'], label='Training Accuracy')\n","plt.plot(history.history['val_accuracy'] + fine_tune_history.history['val_accuracy'], label='Validation Accuracy')\n","plt.title('Model Accuracy')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","\n","# Plot loss\n","plt.subplot(1, 2, 2)\n","plt.plot(history.history['loss'] + fine_tune_history.history['loss'], label='Training Loss')\n","plt.plot(history.history['val_loss'] + fine_tune_history.history['val_loss'], label='Validation Loss')\n","plt.title('Model Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","plt.tight_layout()\n","plt.savefig('transfer_learning_history.png')"]},{"cell_type":"markdown","metadata":{"id":"9ab7rsD0RRdv"},"source":["## Visualizing Predictions\n","\n","Let's make predictions on the test set and visualize some examples to assess the model's performance qualitatively."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SiL48tktRRdv"},"outputs":[],"source":["# Make predictions\n","predictions = model.predict(X_test)\n","predicted_classes = np.argmax(predictions, axis=1)\n","true_classes = np.argmax(y_test, axis=1)\n","\n","# Visualize some predictions\n","plt.figure(figsize=(12, 4))\n","for i in range(5):\n","    plt.subplot(1, 5, i+1)\n","    # Undo VGG16 preprocessing for visualization\n","    img = X_test[i].copy()\n","    img[:, :, 0] += 103.939\n","    img[:, :, 1] += 116.779\n","    img[:, :, 2] += 123.68\n","    img = img[:, :, ::-1]  # BGR to RGB\n","    img = np.clip(img, 0, 255).astype('uint8')\n","    plt.imshow(img)\n","    plt.title(f\"Pred: {class_names[predicted_classes[i]]}\\nTrue: {class_names[true_classes[i]]}\")\n","    plt.axis('off')\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"alLCjgFORRdw"},"source":["## Explanation\n","\n","- **Transfer Learning**: We used VGG16 pre-trained on ImageNet, leveraging its learned features for CIFAR-10 classification.\n","- **Feature Extraction**: Initially, we froze the VGG16 layers and trained only the custom dense layers to adapt to the new task.\n","- **Fine-Tuning**: We unfroze the last convolutional block (block5) and fine-tuned it with a low learning rate to improve performance.\n","- **Preprocessing**: Images were resized to 224x224 and preprocessed to match VGG16's requirements (BGR format, specific mean subtraction).\n","- **Model Architecture**: Added global average pooling, a dense layer with ReLU, dropout for regularization, and a softmax output for 10 classes.\n","- **Evaluation**: Assessed performance with test accuracy/loss and visualized training history to monitor convergence.\n","- **Predictions**: Visualized sample predictions to qualitatively evaluate the model.\n","\n","To extend this work, consider:\n","- Using other pre-trained models like ResNet50 or EfficientNet\n","- Applying data augmentation (e.g., random flips, rotations) to improve robustness\n","- Fine-tuning more layers or adjusting the learning rate schedule\n","- Experimenting with different custom top layers or regularization techniques"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}