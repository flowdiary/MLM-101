{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 66: Transfer Learning in Deep Learning\n",
    "\n",
    "This notebook demonstrates **transfer learning** using a pre-trained **ResNet50** model for image classification on the CIFAR-10 dataset. Transfer learning involves using a model pre-trained on a large dataset (e.g., ImageNet) and adapting it for a new task. We'll cover:\n",
    "\n",
    "- Loading and preprocessing the CIFAR-10 dataset\n",
    "- Using a pre-trained ResNet50 model (excluding top layers)\n",
    "- Adding custom layers for the new task\n",
    "- Freezing and fine-tuning the model\n",
    "- Training and evaluating the model\n",
    "- Visualizing predictions\n",
    "\n",
    "ResNet50, pre-trained on ImageNet, will be used as the base model, with fine-tuning to adapt it to CIFAR-10's 10 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "Let's import the necessary libraries and set up the environment for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Preprocessing the CIFAR-10 Dataset\n",
    "\n",
    "CIFAR-10 contains 60,000 32x32 RGB images across 10 classes. We'll preprocess the images to match ResNet50's expected input format (resizing to 224x224 and applying ResNet50-specific preprocessing) and convert labels to one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10 dataset\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Class names for CIFAR-10\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# Resize images to 224x224 for ResNet50\n",
    "def resize_images(images, target_size=(224, 224)):\n",
    "    resized_images = np.zeros((images.shape[0], *target_size, 3))\n",
    "    for i in range(images.shape[0]):\n",
    "        resized_images[i] = tf.image.resize(images[i], target_size).numpy()\n",
    "    return resized_images\n",
    "\n",
    "X_train = resize_images(X_train)\n",
    "X_test = resize_images(X_test)\n",
    "\n",
    "# Normalize pixel values using ResNet50 preprocessing\n",
    "X_train = keras.applications.resnet50.preprocess_input(X_train)\n",
    "X_test = keras.applications.resnet50.preprocess_input(X_test)\n",
    "\n",
    "# Convert labels to one-hit encoding\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "\n",
    "# Visualize some sample images\n",
    "plt.figure(figsize=(10, 2))\n",
    "for i in range(5):\n",
    "    plt.subplot(1, 5, i+1)\n",
    "    # Undo ResNet50 preprocessing for visualization\n",
    "    img = X_train[i].copy()\n",
    "    img[:, :, 0] += 103.939\n",
    "    img[:, :, 1] += 116.779\n",
    "    img[:, :, 2] += 123.68\n",
    "    img = img[:, :, ::-1]  # BGR to RGB\n",
    "    img = np.clip(img, 0, 255).astype('uint8')\n",
    "    plt.imshow(img)\n",
    "    plt.title(class_names[np.argmax(y_train[i])])\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Pre-trained ResNet50 Model\n",
    "\n",
    "We'll load ResNet50 pre-trained on ImageNet, excluding its top (fully connected) layers, and add custom layers for CIFAR-10 classification. We'll initially freeze the pre-trained layers to use their learned features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ResNet50 without top layers\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the base model layers\n",
    "base_model.trainable = False\n",
    "\n",
    "# Add custom layers\n",
    "inputs = keras.Input(shape=(224, 224, 3))\n",
    "x = base_model(inputs, training=False)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "outputs = Dense(10, activation='softmax')(x)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model (Feature Extraction)\n",
    "\n",
    "First, we'll train the model with the pre-trained ResNet50 layers frozen, only updating the custom layers. This is known as **feature extraction**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define early stopping\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train the model (feature extraction)\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[early_stopping],\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning the Model\n",
    "\n",
    "To improve performance, we'll unfreeze some of the later layers of ResNet50 and fine-tune them with a lower learning rate. This allows the model to adapt the pre-trained features to the CIFAR-10 dataset. We'll unfreeze layers from the last residual block (e.g., `conv5_block3`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze the base model\n",
    "base_model.trainable = True\n",
    "\n",
    "# Freeze all layers except the last residual block (conv5_block3)\n",
    "for layer in base_model.layers:\n",
    "    if 'conv5_block3' in layer.name:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "\n",
    "# Recompile the model with a lower learning rate\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-5),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Fine-tune the model\n",
    "fine_tune_history = model.fit(X_train, y_train,\n",
    "                              epochs=10,\n",
    "                              batch_size=32,\n",
    "                              validation_split=0.2,\n",
    "                              callbacks=[early_stopping],\n",
    "                              verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model\n",
    "\n",
    "We'll evaluate the fine-tuned model on the test set and visualize the training and validation accuracy/loss curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test loss: {test_loss:.4f}\")\n",
    "\n",
    "# Plot training history (combine feature extraction and fine-tuning)\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'] + fine_tune_history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'] + fine_tune_history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'] + fine_tune_history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'] + fine_tune_history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('transfer_learning_resnet50_history.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Predictions\n",
    "\n",
    "Let's make predictions on the test set and visualize some examples to assess the model's performance qualitatively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Visualize some predictions\n",
    "plt.figure(figsize=(12, 4))\n",
    "for i in range(5):\n",
    "    plt.subplot(1, 5, i+1)\n",
    "    # Undo ResNet50 preprocessing for visualization\n",
    "    img = X_test[i].copy()\n",
    "    img[:, :, 0] += 103.939\n",
    "    img[:, :, 1] += 116.779\n",
    "    img[:, :, 2] += 123.68\n",
    "    img = img[:, :, ::-1]  # BGR to RGB\n",
    "    img = np.clip(img, 0, 255).astype('uint8')\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Pred: {class_names[predicted_classes[i]]}\\nTrue: {class_names[true_classes[i]]}\")\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation\n",
    "\n",
    "- **Transfer Learning**: We used ResNet50 pre-trained on ImageNet, leveraging its learned features for CIFAR-10 classification.\n",
    "- **Feature Extraction**: Initially, we froze the ResNet50 layers and trained only the custom dense layers to adapt to the new task.\n",
    "- **Fine-Tuning**: We unfroze the last residual block (conv5_block3) and fine-tuned it with a low learning rate to improve performance.\n",
    "- **Preprocessing**: Images were resized to 224x224 and preprocessed to match ResNet50's requirements (BGR format, specific mean subtraction).\n",
    "- **Model Architecture**: Added global average pooling, a dense layer with ReLU, dropout for regularization, and a softmax output for 10 classes.\n",
    "- **Evaluation**: Assessed performance with test accuracy/loss and visualized training history to monitor convergence.\n",
    "- **Predictions**: Visualized sample predictions to qualitatively evaluate the model.\n",
    "\n",
    "To extend this work, consider:\n",
    "- Using other pre-trained models like EfficientNet or VGG16\n",
    "- Applying data augmentation (e.g., random flips, rotations) to improve robustness\n",
    "- Fine-tuning more layers or adjusting the learning rate schedule\n",
    "- Experimenting with different custom top layers or regularization techniques"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}