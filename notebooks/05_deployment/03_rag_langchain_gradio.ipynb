{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7efcde6c",
   "metadata": {},
   "source": [
    "# Lecture 83 â€“ RAG with LangChain and Gradio\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand Retrieval-Augmented Generation (RAG)\n",
    "- Build a semantic search system with embeddings\n",
    "- Create a FAISS vector database\n",
    "- Implement a Gradio UI for RAG\n",
    "- Integrate with language models\n",
    "\n",
    "## Expected Runtime\n",
    "~7 minutes (includes model downloads)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6238d39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sentence-transformers faiss-cpu gradio langchain transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5124070f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from typing import List, Tuple\n",
    "\n",
    "print(\"âœ“ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f2d238",
   "metadata": {},
   "source": [
    "## 1. Create Sample Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0a5062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample corpus about ML deployment\n",
    "DOCUMENTS = [\n",
    "    \"Deep learning uses neural networks with multiple layers for complex pattern recognition.\",\n",
    "    \"Model deployment involves serializing trained models and serving them via APIs.\",\n",
    "    \"FastAPI is a high-performance Python framework ideal for ML model serving.\",\n",
    "    \"Docker containers ensure consistent deployment across different environments.\",\n",
    "    \"RAG systems combine retrieval and generation for accurate, grounded responses.\",\n",
    "    \"TensorFlow and PyTorch are the leading deep learning frameworks.\",\n",
    "    \"Monitoring model performance in production is crucial for maintaining quality.\",\n",
    "    \"API versioning allows smooth transitions between model updates.\"\n",
    "]\n",
    "\n",
    "print(f\"Knowledge base: {len(DOCUMENTS)} documents\")\n",
    "for i, doc in enumerate(DOCUMENTS, 1):\n",
    "    print(f\"{i}. {doc[:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6ec309",
   "metadata": {},
   "source": [
    "## 2. Create Embeddings with Sentence Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2369d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embedding model\n",
    "print(\"Loading embedding model...\")\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "print(f\"Model dimension: {embedder.get_sentence_embedding_dimension()}\")\n",
    "print(\"âœ“ Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c982f387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings\n",
    "print(\"Encoding documents...\")\n",
    "embeddings = embedder.encode(DOCUMENTS, show_progress_bar=True)\n",
    "embeddings = np.array(embeddings).astype('float32')\n",
    "\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"Sample embedding (first 10 values): {embeddings[0][:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c52a750",
   "metadata": {},
   "source": [
    "## 3. Build FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c12f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FAISS index\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)  # L2 distance\n",
    "\n",
    "# Add vectors to index\n",
    "index.add(embeddings)\n",
    "\n",
    "print(f\"âœ“ FAISS index created\")\n",
    "print(f\"  Vectors in index: {index.ntotal}\")\n",
    "print(f\"  Index dimension: {dimension}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdebc91",
   "metadata": {},
   "source": [
    "## 4. Semantic Search Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f185449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query: str, top_k: int = 3) -> List[Tuple[str, float]]:\n",
    "    \"\"\"Perform semantic search and return top-k results.\"\"\"\n",
    "    # Encode query\n",
    "    query_embedding = embedder.encode([query]).astype('float32')\n",
    "    \n",
    "    # Search index\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    \n",
    "    # Format results\n",
    "    results = []\n",
    "    for idx, dist in zip(indices[0], distances[0]):\n",
    "        results.append((DOCUMENTS[idx], float(dist)))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test\n",
    "test_query = \"How do I deploy a model?\"\n",
    "results = semantic_search(test_query, top_k=3)\n",
    "\n",
    "print(f\"Query: '{test_query}'\\n\")\n",
    "for i, (doc, score) in enumerate(results, 1):\n",
    "    print(f\"{i}. [{score:.4f}] {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d38fc3",
   "metadata": {},
   "source": [
    "## 5. RAG Pipeline with Mock LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba19f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(query: str, context_docs: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Generate answer using retrieved context.\n",
    "    In production, replace with actual LLM API call.\n",
    "    \"\"\"\n",
    "    context = \"\\n\".join([f\"- {doc}\" for doc in context_docs])\n",
    "    \n",
    "    # Mock response (replace with OpenAI/Anthropic/HuggingFace API)\n",
    "    answer = f\"\"\"**Question:** {query}\n",
    "\n",
    "**Retrieved Context:**\n",
    "{context}\n",
    "\n",
    "**Generated Answer:**\n",
    "Based on the retrieved information, here's what you need to know:\n",
    "\n",
    "{context_docs[0]}\n",
    "\n",
    "This demonstrates a RAG system that retrieves relevant information and uses it to generate contextual answers. In production, this would use a large language model like GPT-4, Claude, or an open-source model to generate more sophisticated responses.\n",
    "\n",
    "*Note: To use a real LLM, set your API key and integrate the model here.*\n",
    "\"\"\"\n",
    "    return answer\n",
    "\n",
    "def rag_pipeline(query: str, top_k: int = 3) -> Tuple[str, List[str]]:\n",
    "    \"\"\"Complete RAG: retrieve + generate.\"\"\"\n",
    "    # Retrieve\n",
    "    results = semantic_search(query, top_k)\n",
    "    docs = [doc for doc, _ in results]\n",
    "    \n",
    "    # Generate\n",
    "    answer = generate_answer(query, docs)\n",
    "    \n",
    "    return answer, docs\n",
    "\n",
    "# Test RAG pipeline\n",
    "answer, sources = rag_pipeline(\"What is RAG?\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725def11",
   "metadata": {},
   "source": [
    "## 6. Create Gradio Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d77998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def gradio_rag(question: str, num_sources: int = 3):\n",
    "    \"\"\"Gradio interface function.\"\"\"\n",
    "    if not question.strip():\n",
    "        return \"Please enter a question.\", \"\"\n",
    "    \n",
    "    answer, sources = rag_pipeline(question, num_sources)\n",
    "    sources_text = \"\\n\\n\".join([f\"ðŸ“„ {i+1}. {s}\" for i, s in enumerate(sources)])\n",
    "    \n",
    "    return answer, sources_text\n",
    "\n",
    "# Create interface\n",
    "with gr.Blocks(title=\"RAG Demo\") as demo:\n",
    "    gr.Markdown(\"# ðŸ¤– RAG System Demo\")\n",
    "    gr.Markdown(\"Ask questions about ML deployment!\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            question = gr.Textbox(label=\"Question\", lines=3)\n",
    "            num_sources = gr.Slider(1, 5, 3, step=1, label=\"Sources\")\n",
    "            submit = gr.Button(\"Ask\")\n",
    "        \n",
    "        with gr.Column():\n",
    "            answer = gr.Textbox(label=\"Answer\", lines=10)\n",
    "            sources = gr.Textbox(label=\"Sources\", lines=5)\n",
    "    \n",
    "    gr.Examples(\n",
    "        [[\"What is deep learning?\"],\n",
    "         [\"How do I deploy a model?\"],\n",
    "         [\"What is RAG?\"]],\n",
    "        inputs=question\n",
    "    )\n",
    "    \n",
    "    submit.click(gradio_rag, [question, num_sources], [answer, sources])\n",
    "\n",
    "# Launch (uncomment to run)\n",
    "# demo.launch(share=True)\n",
    "print(\"âœ“ Gradio interface created\")\n",
    "print(\"  Uncomment demo.launch() to start the UI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cc2be3",
   "metadata": {},
   "source": [
    "## 7. Integration with Real LLMs\n",
    "\n",
    "### OpenAI Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5213696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example OpenAI integration (requires API key)\n",
    "example_code = '''\n",
    "import openai\n",
    "import os\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "def generate_with_openai(query: str, context_docs: List[str]) -> str:\n",
    "    context = \"\\\\n\".join(context_docs)\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant. Answer based on the provided context.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Context:\\\\n{context}\\\\n\\\\nQuestion: {query}\"}\n",
    "    ]\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=messages,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "'''\n",
    "\n",
    "print(\"OpenAI Integration Example:\")\n",
    "print(example_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321ae22e",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "âœ“ Built semantic search with sentence-transformers  \n",
    "âœ“ Created FAISS vector database  \n",
    "âœ“ Implemented RAG pipeline  \n",
    "âœ“ Created Gradio UI  \n",
    "\n",
    "### Production Checklist:\n",
    "- [ ] Use production vector DB (Pinecone, Weaviate, Qdrant)\n",
    "- [ ] Integrate real LLM (OpenAI, Anthropic, or open-source)\n",
    "- [ ] Add caching for embeddings\n",
    "- [ ] Implement rate limiting\n",
    "- [ ] Monitor retrieval quality\n",
    "- [ ] Add feedback collection\n",
    "\n",
    "**Next**: `04_docker_and_containerization.ipynb`"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
