{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "041e07e7",
   "metadata": {},
   "source": [
    "# Lecture 83 – Serving Models with FastAPI\n",
    "\n",
    "## Learning Objectives\n",
    "- Build a production-ready REST API for model serving\n",
    "- Use FastAPI for high-performance inference endpoints\n",
    "- Implement request validation with Pydantic\n",
    "- Add health checks and metadata endpoints\n",
    "- Test API with cURL and Python requests\n",
    "\n",
    "## Expected Runtime\n",
    "~3-5 minutes (API server runs in background)\n",
    "\n",
    "## Prerequisites\n",
    "- Completed notebook 01 (saved model artifacts)\n",
    "- FastAPI, Uvicorn, Pydantic\n",
    "- Python requests library for testing\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92ecfb9",
   "metadata": {},
   "source": [
    "## Setup and Environment Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4589c6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install fastapi==0.104.1 uvicorn[standard]==0.24.0 pydantic==2.5.0 python-multipart requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1c095c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    import fastapi\n",
    "    import uvicorn\n",
    "    import pydantic\n",
    "    import requests\n",
    "    print(f\"✓ FastAPI version: {fastapi.__version__}\")\n",
    "    print(f\"✓ Uvicorn version: {uvicorn.__version__}\")\n",
    "    print(f\"✓ Pydantic version: {pydantic.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"Missing package: {e}\")\n",
    "    print(\"Run: pip install fastapi uvicorn[standard] pydantic requests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a8df8b",
   "metadata": {},
   "source": [
    "## 1. Review the FastAPI Application Structure\n",
    "\n",
    "We've created a complete FastAPI application in `apps/fastapi_app/`. Let's examine it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3f7772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the FastAPI app exists\n",
    "app_path = Path('../apps/fastapi_app/app.py')\n",
    "\n",
    "if app_path.exists():\n",
    "    print(f\"✓ FastAPI app found at: {app_path}\")\n",
    "    print(f\"\\nApp structure (first 50 lines):\")\n",
    "    with open(app_path, 'r') as f:\n",
    "        lines = f.readlines()[:50]\n",
    "        print(''.join(lines))\n",
    "else:\n",
    "    print(f\"✗ App not found. Please run notebook 01 first or create the app.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ab5d63",
   "metadata": {},
   "source": [
    "## 2. Understanding the API Endpoints\n",
    "\n",
    "Our FastAPI application provides:\n",
    "\n",
    "1. **GET /ping** - Health check endpoint\n",
    "2. **POST /predict** - Main inference endpoint\n",
    "3. **GET /metadata** - Model information\n",
    "4. **GET /** - Root endpoint with API documentation\n",
    "\n",
    "### Key Features:\n",
    "- **Pydantic Models**: Type-safe request/response validation\n",
    "- **Async Support**: Non-blocking I/O for better performance\n",
    "- **Error Handling**: Proper HTTP status codes and error messages\n",
    "- **Auto Documentation**: Swagger UI at `/docs`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a497b8",
   "metadata": {},
   "source": [
    "## 3. Starting the FastAPI Server\n",
    "\n",
    "**Note**: We'll start the server in a separate terminal or background process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e190dc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple test to check if server is running\n",
    "import requests\n",
    "import time\n",
    "\n",
    "def check_server_running(url=\"http://localhost:8000\", max_retries=3):\n",
    "    \"\"\"Check if the FastAPI server is running.\"\"\"\n",
    "    for i in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(f\"{url}/ping\", timeout=2)\n",
    "            if response.status_code == 200:\n",
    "                print(f\"✓ Server is running at {url}\")\n",
    "                return True\n",
    "        except requests.exceptions.RequestException:\n",
    "            if i < max_retries - 1:\n",
    "                print(f\"Waiting for server... (attempt {i+1}/{max_retries})\")\n",
    "                time.sleep(2)\n",
    "    \n",
    "    print(f\"✗ Server not running at {url}\")\n",
    "    print(\"\\nTo start the server, run in a terminal:\")\n",
    "    print(\"  cd apps/fastapi_app\")\n",
    "    print(\"  uvicorn app:app --host 0.0.0.0 --port 8000 --reload\")\n",
    "    return False\n",
    "\n",
    "SERVER_URL = \"http://localhost:8000\"\n",
    "server_running = check_server_running(SERVER_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d89661",
   "metadata": {},
   "source": [
    "## 4. Testing API Endpoints\n",
    "\n",
    "### 4.1 Health Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77acf59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if server_running:\n",
    "    response = requests.get(f\"{SERVER_URL}/ping\")\n",
    "    print(f\"Status Code: {response.status_code}\")\n",
    "    print(f\"Response: {response.json()}\")\n",
    "else:\n",
    "    print(\"Server not running. Example response:\")\n",
    "    print('{\"status\": \"healthy\", \"model_loaded\": true}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41b73dd",
   "metadata": {},
   "source": [
    "### 4.2 Get Model Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c6fb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "if server_running:\n",
    "    response = requests.get(f\"{SERVER_URL}/metadata\")\n",
    "    print(f\"Status Code: {response.status_code}\")\n",
    "    print(f\"\\nModel Metadata:\")\n",
    "    print(json.dumps(response.json(), indent=2))\n",
    "else:\n",
    "    print(\"Server not running. Example metadata:\")\n",
    "    example_metadata = {\n",
    "        \"model_name\": \"fashion_mnist_cnn\",\n",
    "        \"version\": \"20241102_120000\",\n",
    "        \"input_shape\": [28, 28, 1],\n",
    "        \"output_classes\": 10,\n",
    "        \"class_names\": [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "                       \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "    }\n",
    "    print(json.dumps(example_metadata, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3105be91",
   "metadata": {},
   "source": [
    "### 4.3 Make Predictions\n",
    "\n",
    "Let's create a test image and send it to the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3ba8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a test image from Fashion-MNIST\n",
    "import tensorflow as tf\n",
    "\n",
    "(_, _), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# Get a few test images (raw, not normalized)\n",
    "test_images = x_test[:3].tolist()  # Convert to list for JSON serialization\n",
    "\n",
    "print(f\"Loaded {len(test_images)} test images\")\n",
    "print(f\"Image shape: {np.array(test_images[0]).shape}\")\n",
    "print(f\"Actual labels: {y_test[:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9761f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the request payload\n",
    "payload = {\n",
    "    \"instances\": test_images\n",
    "}\n",
    "\n",
    "print(f\"Payload contains {len(payload['instances'])} images\")\n",
    "print(f\"Payload size: ~{len(json.dumps(payload)) / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe9d14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if server_running:\n",
    "    # Send prediction request\n",
    "    response = requests.post(\n",
    "        f\"{SERVER_URL}/predict\",\n",
    "        json=payload,\n",
    "        headers={\"Content-Type\": \"application/json\"}\n",
    "    )\n",
    "    \n",
    "    print(f\"Status Code: {response.status_code}\")\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        print(f\"\\nPredictions:\")\n",
    "        for i, pred in enumerate(result['predictions']):\n",
    "            print(f\"  Image {i+1}: {pred['class_name']} (confidence: {pred['confidence']:.4f})\")\n",
    "            print(f\"           Actual: {['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'][y_test[i]]}\")\n",
    "    else:\n",
    "        print(f\"Error: {response.text}\")\n",
    "else:\n",
    "    print(\"Server not running. Example prediction response:\")\n",
    "    example_response = {\n",
    "        \"predictions\": [\n",
    "            {\"class_id\": 9, \"class_name\": \"Ankle boot\", \"confidence\": 0.9876},\n",
    "            {\"class_id\": 2, \"class_name\": \"Pullover\", \"confidence\": 0.8543},\n",
    "            {\"class_id\": 1, \"class_name\": \"Trouser\", \"confidence\": 0.9234}\n",
    "        ],\n",
    "        \"model_version\": \"20241102_120000\"\n",
    "    }\n",
    "    print(json.dumps(example_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187affa4",
   "metadata": {},
   "source": [
    "## 5. Testing with cURL\n",
    "\n",
    "You can also test the API using cURL commands from the terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8beb897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate example cURL commands\n",
    "curl_commands = f\"\"\"\n",
    "# Health check\n",
    "curl -X GET \"{SERVER_URL}/ping\"\n",
    "\n",
    "# Get metadata\n",
    "curl -X GET \"{SERVER_URL}/metadata\"\n",
    "\n",
    "# Make prediction (with single image)\n",
    "curl -X POST \"{SERVER_URL}/predict\" \\\\\n",
    "  -H \"Content-Type: application/json\" \\\\\n",
    "  -d '{{\n",
    "    \"instances\": [[0, 0, 0, ..., 0]]  # 28x28 = 784 values\n",
    "  }}'\n",
    "\n",
    "# View interactive documentation\n",
    "open {SERVER_URL}/docs\n",
    "\"\"\"\n",
    "\n",
    "print(\"Example cURL commands:\")\n",
    "print(curl_commands)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cf1982",
   "metadata": {},
   "source": [
    "## 6. Performance Testing\n",
    "\n",
    "Let's measure API latency and throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3f2f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_api(url, num_requests=10, batch_size=1):\n",
    "    \"\"\"Benchmark API performance.\"\"\"\n",
    "    if not server_running:\n",
    "        print(\"Server not running. Skipping benchmark.\")\n",
    "        return\n",
    "    \n",
    "    latencies = []\n",
    "    \n",
    "    print(f\"Running {num_requests} requests with batch_size={batch_size}...\")\n",
    "    \n",
    "    for i in range(num_requests):\n",
    "        # Prepare batch\n",
    "        batch = x_test[:batch_size].tolist()\n",
    "        payload = {\"instances\": batch}\n",
    "        \n",
    "        # Measure latency\n",
    "        start = time.time()\n",
    "        response = requests.post(f\"{url}/predict\", json=payload)\n",
    "        latency = (time.time() - start) * 1000  # Convert to ms\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            latencies.append(latency)\n",
    "        else:\n",
    "            print(f\"Request {i+1} failed: {response.status_code}\")\n",
    "    \n",
    "    if latencies:\n",
    "        print(f\"\\nPerformance Metrics:\")\n",
    "        print(f\"  Total requests: {len(latencies)}\")\n",
    "        print(f\"  Mean latency: {np.mean(latencies):.2f} ms\")\n",
    "        print(f\"  Median latency: {np.median(latencies):.2f} ms\")\n",
    "        print(f\"  95th percentile: {np.percentile(latencies, 95):.2f} ms\")\n",
    "        print(f\"  Min latency: {np.min(latencies):.2f} ms\")\n",
    "        print(f\"  Max latency: {np.max(latencies):.2f} ms\")\n",
    "        print(f\"  Throughput: {1000 / np.mean(latencies):.2f} requests/sec\")\n",
    "\n",
    "benchmark_api(SERVER_URL, num_requests=20, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce71baa",
   "metadata": {},
   "source": [
    "## 7. Error Handling and Validation\n",
    "\n",
    "Let's test how the API handles invalid requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3036eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "if server_running:\n",
    "    # Test 1: Empty request\n",
    "    print(\"Test 1: Empty request\")\n",
    "    response = requests.post(f\"{SERVER_URL}/predict\", json={})\n",
    "    print(f\"  Status: {response.status_code}\")\n",
    "    print(f\"  Response: {response.json()}\\n\")\n",
    "    \n",
    "    # Test 2: Wrong shape\n",
    "    print(\"Test 2: Wrong image shape\")\n",
    "    response = requests.post(f\"{SERVER_URL}/predict\", \n",
    "                            json={\"instances\": [[1, 2, 3]]})  # Too small\n",
    "    print(f\"  Status: {response.status_code}\")\n",
    "    print(f\"  Response: {response.json()}\\n\")\n",
    "    \n",
    "    # Test 3: Invalid data type\n",
    "    print(\"Test 3: Invalid data type\")\n",
    "    response = requests.post(f\"{SERVER_URL}/predict\", \n",
    "                            json={\"instances\": \"not a list\"})\n",
    "    print(f\"  Status: {response.status_code}\")\n",
    "    print(f\"  Response: {response.json()}\")\n",
    "else:\n",
    "    print(\"Server not running. Example error responses:\")\n",
    "    print(\"422 Unprocessable Entity: {\\\"detail\\\": \\\"Invalid input format\\\"}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b4bbd4",
   "metadata": {},
   "source": [
    "## 8. Production Deployment Notes\n",
    "\n",
    "### 8.1 Running with Gunicorn (Production Server)\n",
    "\n",
    "```bash\n",
    "# Install gunicorn\n",
    "pip install gunicorn\n",
    "\n",
    "# Run with multiple workers\n",
    "gunicorn app:app \\\n",
    "  --workers 4 \\\n",
    "  --worker-class uvicorn.workers.UvicornWorker \\\n",
    "  --bind 0.0.0.0:8000 \\\n",
    "  --timeout 60 \\\n",
    "  --access-logfile - \\\n",
    "  --error-logfile -\n",
    "```\n",
    "\n",
    "### 8.2 Environment Variables\n",
    "\n",
    "```bash\n",
    "export MODEL_PATH=/path/to/saved/model\n",
    "export PREPROCESSING_PATH=/path/to/preprocessing.pkl\n",
    "export LOG_LEVEL=INFO\n",
    "export MAX_BATCH_SIZE=32\n",
    "```\n",
    "\n",
    "### 8.3 Monitoring and Logging\n",
    "\n",
    "- Use structured logging (JSON format)\n",
    "- Add request IDs for tracing\n",
    "- Implement metrics endpoint for Prometheus\n",
    "- Set up alerts for error rates and latency\n",
    "\n",
    "### 8.4 Security Considerations\n",
    "\n",
    "- Add API key authentication\n",
    "- Implement rate limiting\n",
    "- Use HTTPS in production\n",
    "- Validate and sanitize all inputs\n",
    "- Set maximum request size limits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde0f355",
   "metadata": {},
   "source": [
    "## 9. Writing Unit Tests\n",
    "\n",
    "Here's an example test suite using pytest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d1f670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test file\n",
    "test_code = '''\n",
    "\"\"\"Unit tests for FastAPI model serving.\"\"\"\n",
    "import pytest\n",
    "from fastapi.testclient import TestClient\n",
    "import numpy as np\n",
    "\n",
    "# Assuming app.py is in the same directory\n",
    "from app import app\n",
    "\n",
    "client = TestClient(app)\n",
    "\n",
    "\n",
    "def test_ping():\n",
    "    \"\"\"Test health check endpoint.\"\"\"\n",
    "    response = client.get(\"/ping\")\n",
    "    assert response.status_code == 200\n",
    "    assert response.json()[\"status\"] == \"healthy\"\n",
    "\n",
    "\n",
    "def test_metadata():\n",
    "    \"\"\"Test metadata endpoint.\"\"\"\n",
    "    response = client.get(\"/metadata\")\n",
    "    assert response.status_code == 200\n",
    "    data = response.json()\n",
    "    assert \"model_name\" in data\n",
    "    assert \"input_shape\" in data\n",
    "\n",
    "\n",
    "def test_predict_valid():\n",
    "    \"\"\"Test prediction with valid input.\"\"\"\n",
    "    # Create a random 28x28 image\n",
    "    image = np.random.randint(0, 255, (28, 28)).tolist()\n",
    "    \n",
    "    response = client.post(\"/predict\", json={\"instances\": [image]})\n",
    "    assert response.status_code == 200\n",
    "    \n",
    "    data = response.json()\n",
    "    assert \"predictions\" in data\n",
    "    assert len(data[\"predictions\"]) == 1\n",
    "    assert \"class_name\" in data[\"predictions\"][0]\n",
    "\n",
    "\n",
    "def test_predict_invalid_shape():\n",
    "    \"\"\"Test prediction with invalid input shape.\"\"\"\n",
    "    response = client.post(\"/predict\", json={\"instances\": [[1, 2, 3]]})\n",
    "    assert response.status_code == 400\n",
    "\n",
    "\n",
    "def test_predict_empty():\n",
    "    \"\"\"Test prediction with empty input.\"\"\"\n",
    "    response = client.post(\"/predict\", json={\"instances\": []})\n",
    "    assert response.status_code == 400\n",
    "'''\n",
    "\n",
    "print(\"Example pytest test suite:\")\n",
    "print(test_code)\n",
    "\n",
    "# Save to file\n",
    "test_file = Path('../apps/fastapi_app/test_app.py')\n",
    "with open(test_file, 'w') as f:\n",
    "    f.write(test_code)\n",
    "print(f\"\\n✓ Test file saved to: {test_file}\")\n",
    "print(\"\\nRun tests with: pytest test_app.py -v\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42106aa7",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we:\n",
    "\n",
    "1. ✓ Created a production-ready FastAPI application\n",
    "2. ✓ Implemented health, metadata, and prediction endpoints\n",
    "3. ✓ Added request validation with Pydantic\n",
    "4. ✓ Tested the API with Python and cURL\n",
    "5. ✓ Benchmarked API performance\n",
    "6. ✓ Handled errors gracefully\n",
    "7. ✓ Wrote unit tests with pytest\n",
    "\n",
    "### Production Checklist:\n",
    "\n",
    "- [ ] Use Gunicorn with multiple workers\n",
    "- [ ] Add authentication (API keys, OAuth)\n",
    "- [ ] Implement rate limiting\n",
    "- [ ] Set up monitoring (Prometheus + Grafana)\n",
    "- [ ] Add logging (structured JSON logs)\n",
    "- [ ] Use HTTPS with valid certificates\n",
    "- [ ] Set up CI/CD pipeline\n",
    "- [ ] Implement load balancing\n",
    "- [ ] Add caching for repeated requests\n",
    "- [ ] Document API with OpenAPI/Swagger\n",
    "\n",
    "---\n",
    "\n",
    "## Extension Ideas\n",
    "\n",
    "1. **Batch Optimization**: Add batching logic to process multiple requests together\n",
    "2. **Model Versioning**: Support multiple model versions with A/B testing\n",
    "3. **Async Processing**: Use background tasks for long-running predictions\n",
    "4. **Caching**: Implement Redis caching for frequent predictions\n",
    "5. **Metrics**: Add Prometheus metrics endpoint\n",
    "6. **WebSocket**: Real-time streaming predictions\n",
    "\n",
    "---\n",
    "\n",
    "**Next**: `03_rag_langchain_gradio.ipynb` - Build a RAG system with Gradio UI"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
