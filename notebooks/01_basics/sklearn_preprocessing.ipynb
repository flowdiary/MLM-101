{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd350948",
   "metadata": {},
   "source": [
    "# Scikit-Learn Data Preprocessing\n",
    "\n",
    "**Course:** MLM-101 - Machine Learning Mastery  \n",
    "**Phase 6:** Scikit-Learn Fundamentals (Lectures 43-46)  \n",
    "**Topics:** Scaling, Encoding, Train/Test Split, Pipelines\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "‚úÖ Scale numerical features (StandardScaler, MinMaxScaler)  \n",
    "‚úÖ Encode categorical variables (LabelEncoder, OneHotEncoder)  \n",
    "‚úÖ Split data into train and test sets  \n",
    "‚úÖ Create preprocessing pipelines  \n",
    "‚úÖ Handle missing values with imputers  \n",
    "‚úÖ Prepare data for ML models\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c092c973",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6bdae1",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Feature Scaling\n",
    "\n",
    "Most ML algorithms perform better when features are on similar scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdb663f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data with different scales\n",
    "data = pd.DataFrame({\n",
    "    'age': [25, 30, 35, 40, 45, 50],\n",
    "    'salary': [30000, 45000, 55000, 70000, 85000, 95000],\n",
    "    'years_experience': [1, 3, 5, 8, 12, 15]\n",
    "})\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(data)\n",
    "print(\"\\nStatistics:\")\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbf60e1",
   "metadata": {},
   "source": [
    "### StandardScaler (Z-score Normalization)\n",
    "\n",
    "Transforms features to have mean=0 and std=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa8df41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StandardScaler\n",
    "scaler_standard = StandardScaler()\n",
    "data_standardized = scaler_standard.fit_transform(data)\n",
    "\n",
    "df_standardized = pd.DataFrame(data_standardized, columns=data.columns)\n",
    "\n",
    "print(\"Standardized Data (mean=0, std=1):\")\n",
    "print(df_standardized)\n",
    "print(\"\\nStatistics:\")\n",
    "print(df_standardized.describe())\n",
    "\n",
    "print(\"\\nFormula: (x - mean) / std\")\n",
    "print(f\"Example for age=25: ({25} - {data['age'].mean():.2f}) / {data['age'].std():.2f} = {df_standardized['age'].iloc[0]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0463daee",
   "metadata": {},
   "source": [
    "### MinMaxScaler\n",
    "\n",
    "Scales features to a fixed range [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94994c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinMaxScaler\n",
    "scaler_minmax = MinMaxScaler()\n",
    "data_minmax = scaler_minmax.fit_transform(data)\n",
    "\n",
    "df_minmax = pd.DataFrame(data_minmax, columns=data.columns)\n",
    "\n",
    "print(\"MinMax Scaled Data [0, 1]:\")\n",
    "print(df_minmax)\n",
    "print(\"\\nStatistics:\")\n",
    "print(df_minmax.describe())\n",
    "\n",
    "print(\"\\nFormula: (x - min) / (max - min)\")\n",
    "print(f\"Example for age=25: ({25} - {data['age'].min()}) / ({data['age'].max()} - {data['age'].min()}) = {df_minmax['age'].iloc[0]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e53c23",
   "metadata": {},
   "source": [
    "### üéØ When to Use Which Scaler?\n",
    "\n",
    "- **StandardScaler**: Most common, works well with algorithms assuming normally distributed data (Linear Regression, Logistic Regression, SVM, Neural Networks)\n",
    "- **MinMaxScaler**: When you need features in specific range, good for neural networks with sigmoid/tanh activations\n",
    "- **RobustScaler**: When data has outliers (uses median and IQR instead of mean and std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe5644a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ Encoding Categorical Variables\n",
    "\n",
    "ML algorithms work with numbers, so we need to convert categories to numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf2167a",
   "metadata": {},
   "source": [
    "### Label Encoding\n",
    "\n",
    "Converts categories to integers (0, 1, 2, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2197318b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding for target variable\n",
    "df_labels = pd.DataFrame({\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'David'],\n",
    "    'department': ['Engineering', 'Marketing', 'Engineering', 'Sales'],\n",
    "    'performance': ['Good', 'Excellent', 'Average', 'Good']\n",
    "})\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(df_labels)\n",
    "\n",
    "# Encode department\n",
    "le_dept = LabelEncoder()\n",
    "df_labels['department_encoded'] = le_dept.fit_transform(df_labels['department'])\n",
    "\n",
    "print(\"\\nWith Label Encoding:\")\n",
    "print(df_labels[['department', 'department_encoded']])\n",
    "\n",
    "print(\"\\nMapping:\")\n",
    "for i, label in enumerate(le_dept.classes_):\n",
    "    print(f\"  {label} ‚Üí {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1cd604",
   "metadata": {},
   "source": [
    "### One-Hot Encoding\n",
    "\n",
    "Creates binary columns for each category (better for nominal data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1866605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Encoding\n",
    "df_onehot = pd.DataFrame({\n",
    "    'city': ['New York', 'London', 'Paris', 'New York', 'London']\n",
    "})\n",
    "\n",
    "print(\"Original:\")\n",
    "print(df_onehot)\n",
    "\n",
    "# Using pandas get_dummies\n",
    "df_encoded = pd.get_dummies(df_onehot, columns=['city'], prefix='city')\n",
    "\n",
    "print(\"\\nOne-Hot Encoded:\")\n",
    "print(df_encoded)\n",
    "\n",
    "# Using sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse_output=False)\n",
    "city_encoded = ohe.fit_transform(df_onehot[['city']])\n",
    "\n",
    "print(\"\\nUsing sklearn OneHotEncoder:\")\n",
    "print(city_encoded)\n",
    "print(f\"Categories: {ohe.categories_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de20cce",
   "metadata": {},
   "source": [
    "### üéØ Label Encoding vs One-Hot Encoding\n",
    "\n",
    "- **Label Encoding**: For ordinal data (Low < Medium < High) or target variables\n",
    "- **One-Hot Encoding**: For nominal data (Red, Blue, Green) where no order exists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745b4a17",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc701d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample dataset\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 4)  # 100 samples, 4 features\n",
    "y = np.random.randint(0, 2, 100)  # Binary classification\n",
    "\n",
    "print(f\"Total samples: {len(X)}\")\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06a318f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2,  # 20% for testing\n",
    "    random_state=42,  # For reproducibility\n",
    "    stratify=y  # Maintain class distribution\n",
    ")\n",
    "\n",
    "print(\"Split Results:\")\n",
    "print(f\"  Training samples: {len(X_train)} ({len(X_train)/len(X)*100:.0f}%)\")\n",
    "print(f\"  Test samples: {len(X_test)} ({len(X_test)/len(X)*100:.0f}%)\")\n",
    "\n",
    "print(\"\\nClass Distribution:\")\n",
    "print(f\"  Original: {np.bincount(y)}\")\n",
    "print(f\"  Train: {np.bincount(y_train)}\")\n",
    "print(f\"  Test: {np.bincount(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9107fd2",
   "metadata": {},
   "source": [
    "### üéØ Important: Fit on Train, Transform on Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0b071d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORRECT way to scale\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit only on training data\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Transform both train and test using training statistics\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"‚úÖ CORRECT: Fit on train, transform on both\")\n",
    "print(f\"Train mean: {X_train_scaled.mean(axis=0)}\")\n",
    "print(f\"Test mean: {X_test_scaled.mean(axis=0)}\")\n",
    "print(\"\\n‚ö†Ô∏è Test mean is not exactly 0 because we used training statistics\")\n",
    "\n",
    "# WRONG way (data leakage)\n",
    "print(\"\\n‚ùå WRONG: Fitting on test data causes data leakage!\")\n",
    "print(\"Never do: scaler.fit(X_test) or scaler.fit_transform(X_test)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdcea5b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4Ô∏è‚É£ Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3104568a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data with missing values\n",
    "df_missing = pd.DataFrame({\n",
    "    'age': [25, np.nan, 35, 40, np.nan, 50],\n",
    "    'salary': [50000, 60000, np.nan, 70000, 80000, np.nan],\n",
    "    'experience': [2, 5, 7, np.nan, 12, 15]\n",
    "})\n",
    "\n",
    "print(\"Data with missing values:\")\n",
    "print(df_missing)\n",
    "print(\"\\nMissing count:\")\n",
    "print(df_missing.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9857ce5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimpleImputer with mean strategy\n",
    "imputer_mean = SimpleImputer(strategy='mean')\n",
    "df_imputed_mean = pd.DataFrame(\n",
    "    imputer_mean.fit_transform(df_missing),\n",
    "    columns=df_missing.columns\n",
    ")\n",
    "\n",
    "print(\"Imputed with mean:\")\n",
    "print(df_imputed_mean)\n",
    "\n",
    "# SimpleImputer with median strategy\n",
    "imputer_median = SimpleImputer(strategy='median')\n",
    "df_imputed_median = pd.DataFrame(\n",
    "    imputer_median.fit_transform(df_missing),\n",
    "    columns=df_missing.columns\n",
    ")\n",
    "\n",
    "print(\"\\nImputed with median:\")\n",
    "print(df_imputed_median)\n",
    "\n",
    "# Constant value\n",
    "imputer_constant = SimpleImputer(strategy='constant', fill_value=0)\n",
    "df_imputed_constant = pd.DataFrame(\n",
    "    imputer_constant.fit_transform(df_missing),\n",
    "    columns=df_missing.columns\n",
    ")\n",
    "\n",
    "print(\"\\nImputed with constant (0):\")\n",
    "print(df_imputed_constant)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc96343c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5Ô∏è‚É£ Preprocessing Pipelines\n",
    "\n",
    "Combine multiple preprocessing steps into one pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cbd157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample dataset\n",
    "df = pd.DataFrame({\n",
    "    'age': [25, 30, np.nan, 40, 45],\n",
    "    'salary': [50000, 60000, 70000, np.nan, 90000],\n",
    "    'department': ['Engineering', 'Marketing', 'Engineering', 'Sales', 'Marketing'],\n",
    "    'purchased': [0, 1, 1, 0, 1]\n",
    "})\n",
    "\n",
    "print(\"Original Dataset:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac407c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df.drop('purchased', axis=1)\n",
    "y = df['purchased']\n",
    "\n",
    "# Define numeric and categorical columns\n",
    "numeric_features = ['age', 'salary']\n",
    "categorical_features = ['department']\n",
    "\n",
    "# Create transformers for numeric features\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Create transformers for categorical features\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# Combine transformers\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Fit and transform\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "print(\"\\nProcessed Features:\")\n",
    "print(X_processed)\n",
    "print(f\"\\nShape: {X.shape} ‚Üí {X_processed.shape}\")\n",
    "print(\"(2 numeric + 3 one-hot encoded categorical = 5 features)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbeccf2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Complete ML Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b2be77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create realistic dataset\n",
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "\n",
    "df_complete = pd.DataFrame({\n",
    "    'age': np.random.randint(18, 65, n_samples),\n",
    "    'income': np.random.randint(20000, 150000, n_samples),\n",
    "    'credit_score': np.random.randint(300, 850, n_samples),\n",
    "    'employment': np.random.choice(['Employed', 'Self-Employed', 'Unemployed'], n_samples),\n",
    "    'education': np.random.choice(['High School', 'Bachelor', 'Master', 'PhD'], n_samples),\n",
    "    'loan_approved': np.random.randint(0, 2, n_samples)\n",
    "})\n",
    "\n",
    "# Add some missing values\n",
    "df_complete.loc[np.random.choice(df_complete.index, 20), 'age'] = np.nan\n",
    "df_complete.loc[np.random.choice(df_complete.index, 15), 'income'] = np.nan\n",
    "df_complete.loc[np.random.choice(df_complete.index, 10), 'employment'] = np.nan\n",
    "\n",
    "print(\"Dataset Info:\")\n",
    "print(df_complete.info())\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df_complete.isnull().sum())\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df_complete.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03eecf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X = df_complete.drop('loan_approved', axis=1)\n",
    "y = df_complete['loan_approved']\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "\n",
    "# Define feature types\n",
    "numeric_features = ['age', 'income', 'credit_score']\n",
    "categorical_features = ['employment', 'education']\n",
    "\n",
    "# Numeric pipeline\n",
    "numeric_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical pipeline\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# Complete preprocessor\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_pipeline, numeric_features),\n",
    "    ('cat', categorical_pipeline, categorical_features)\n",
    "])\n",
    "\n",
    "# Fit on train, transform both\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "print(\"\\n‚úÖ Preprocessing Complete!\")\n",
    "print(f\"\\nTrain shape: {X_train.shape} ‚Üí {X_train_processed.shape}\")\n",
    "print(f\"Test shape: {X_test.shape} ‚Üí {X_test_processed.shape}\")\n",
    "print(f\"\\nFeatures breakdown:\")\n",
    "print(f\"  - Numeric: {len(numeric_features)} features\")\n",
    "print(f\"  - Categorical (one-hot): {X_train_processed.shape[1] - len(numeric_features)} features\")\n",
    "print(f\"  - Total: {X_train_processed.shape[1]} features\")\n",
    "\n",
    "print(\"\\nüéØ Data is now ready for ML models!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a09e57",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "‚úÖ **Feature Scaling**: StandardScaler (mean=0, std=1), MinMaxScaler ([0,1])  \n",
    "‚úÖ **Encoding**: LabelEncoder for ordinal, OneHotEncoder for nominal  \n",
    "‚úÖ **Train/Test Split**: Proper data splitting with stratification  \n",
    "‚úÖ **Missing Values**: SimpleImputer with mean/median/most_frequent strategies  \n",
    "‚úÖ **Pipelines**: Automated preprocessing with Pipeline and ColumnTransformer  \n",
    "‚úÖ **Best Practices**: Fit on train, transform on test (avoid data leakage)\n",
    "\n",
    "### ‚ö†Ô∏è Critical Rules\n",
    "\n",
    "1. **Always split BEFORE preprocessing**\n",
    "2. **Fit only on training data**\n",
    "3. **Use same preprocessing for test data**\n",
    "4. **Never use test statistics (causes data leakage)**\n",
    "\n",
    "### üöÄ Next Steps\n",
    "\n",
    "Continue to:\n",
    "- Apply these techniques to real datasets\n",
    "- Build complete ML models with preprocessing pipelines\n",
    "- Experiment with different scaling and encoding strategies\n",
    "\n",
    "---\n",
    "\n",
    "**Course:** MLM-101 - Machine Learning Mastery  \n",
    "**Website:** [https://flowdiary.com.ng/course/MLM-101](https://flowdiary.com.ng/course/MLM-101)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
