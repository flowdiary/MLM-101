{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03f307d4",
   "metadata": {},
   "source": [
    "# NumPy Linear Algebra for Machine Learning\n",
    "\n",
    "**Course:** MLM-101 - Machine Learning Mastery  \n",
    "**Phase 3:** NumPy for Data Computing (Lectures 28-31)  \n",
    "**Topics:** Matrix Operations, Dot Products, Linear Algebra\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "‚úÖ Perform matrix multiplication and dot products  \n",
    "‚úÖ Calculate matrix inverses and determinants  \n",
    "‚úÖ Solve linear equations  \n",
    "‚úÖ Compute eigenvalues and eigenvectors  \n",
    "‚úÖ Apply linear algebra to ML algorithms\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45e621f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672ba21b",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Matrix Multiplication\n",
    "\n",
    "The foundation of neural networks and many ML algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf56870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix multiplication using @\n",
    "A = np.array([[1, 2],\n",
    "              [3, 4]])\n",
    "B = np.array([[5, 6],\n",
    "              [7, 8]])\n",
    "\n",
    "print(\"Matrix A:\")\n",
    "print(A)\n",
    "print(\"\\nMatrix B:\")\n",
    "print(B)\n",
    "\n",
    "# Method 1: @ operator\n",
    "C1 = A @ B\n",
    "print(\"\\nA @ B:\")\n",
    "print(C1)\n",
    "\n",
    "# Method 2: np.dot()\n",
    "C2 = np.dot(A, B)\n",
    "print(\"\\nnp.dot(A, B):\")\n",
    "print(C2)\n",
    "\n",
    "# Method 3: np.matmul()\n",
    "C3 = np.matmul(A, B)\n",
    "print(\"\\nnp.matmul(A, B):\")\n",
    "print(C3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c8e49b",
   "metadata": {},
   "source": [
    "### üéØ ML Example: Forward Pass in Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c420f4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple neural network layer: output = input @ weights + bias\n",
    "np.random.seed(42)\n",
    "\n",
    "# Input: 3 samples, 4 features each\n",
    "X = np.array([[1.0, 2.0, 3.0, 4.0],\n",
    "              [5.0, 6.0, 7.0, 8.0],\n",
    "              [9.0, 10.0, 11.0, 12.0]])\n",
    "\n",
    "# Weights: 4 input features -> 2 output neurons\n",
    "W = np.random.randn(4, 2)\n",
    "\n",
    "# Bias: one per output neuron\n",
    "b = np.array([0.5, -0.5])\n",
    "\n",
    "# Forward pass\n",
    "output = X @ W + b\n",
    "\n",
    "print(f\"Input shape: {X.shape}\")\n",
    "print(f\"Weights shape: {W.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(\"\\nOutput (3 samples, 2 neurons):\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9fbbf4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ Dot Product and Vector Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b53fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dot product of vectors\n",
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5, 6])\n",
    "\n",
    "dot_product = np.dot(a, b)\n",
    "print(f\"a = {a}\")\n",
    "print(f\"b = {b}\")\n",
    "print(f\"\\na ¬∑ b = {dot_product}\")\n",
    "print(f\"Manual: 1*4 + 2*5 + 3*6 = {1*4 + 2*5 + 3*6}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56473fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector length (magnitude)\n",
    "v = np.array([3, 4])\n",
    "magnitude = np.linalg.norm(v)\n",
    "print(f\"Vector: {v}\")\n",
    "print(f\"Magnitude: {magnitude}\")\n",
    "print(f\"Manual: sqrt(3¬≤ + 4¬≤) = {np.sqrt(3**2 + 4**2)}\")\n",
    "\n",
    "# Unit vector (normalization)\n",
    "unit_vector = v / magnitude\n",
    "print(f\"\\nUnit vector: {unit_vector}\")\n",
    "print(f\"Magnitude of unit vector: {np.linalg.norm(unit_vector)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829a7cba",
   "metadata": {},
   "source": [
    "### üéØ ML Example: Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66bf6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine similarity measures angle between vectors\n",
    "# Used in recommendation systems and NLP\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "# User preferences (ratings for 5 items)\n",
    "user1 = np.array([5, 4, 0, 0, 1])\n",
    "user2 = np.array([4, 5, 0, 0, 2])\n",
    "user3 = np.array([0, 0, 5, 4, 0])\n",
    "\n",
    "print(\"User Similarity Matrix:\")\n",
    "print(f\"User 1 vs User 2: {cosine_similarity(user1, user2):.3f} (similar)\")\n",
    "print(f\"User 1 vs User 3: {cosine_similarity(user1, user3):.3f} (different)\")\n",
    "print(f\"User 2 vs User 3: {cosine_similarity(user2, user3):.3f} (different)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dabeb9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ Matrix Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905e7461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose\n",
    "A = np.array([[1, 2, 3],\n",
    "              [4, 5, 6]])\n",
    "\n",
    "print(\"Original (2x3):\")\n",
    "print(A)\n",
    "print(\"\\nTranspose (3x2):\")\n",
    "print(A.T)\n",
    "\n",
    "# Determinant (square matrices only)\n",
    "B = np.array([[1, 2],\n",
    "              [3, 4]])\n",
    "\n",
    "det = np.linalg.det(B)\n",
    "print(f\"\\nMatrix B:\")\n",
    "print(B)\n",
    "print(f\"Determinant: {det}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37fc9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix inverse (for square, non-singular matrices)\n",
    "A = np.array([[4, 7],\n",
    "              [2, 6]])\n",
    "\n",
    "A_inv = np.linalg.inv(A)\n",
    "\n",
    "print(\"Matrix A:\")\n",
    "print(A)\n",
    "print(\"\\nInverse A‚Åª¬π:\")\n",
    "print(A_inv)\n",
    "\n",
    "# Verify: A @ A_inv = Identity\n",
    "identity = A @ A_inv\n",
    "print(\"\\nA @ A‚Åª¬π (should be identity):\")\n",
    "print(np.round(identity, 10))  # Round to avoid floating point errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb4efb2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4Ô∏è‚É£ Solving Linear Equations\n",
    "\n",
    "Solve systems like: Ax = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14b8568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve: 2x + 3y = 8\n",
    "#        x + 4y = 7\n",
    "\n",
    "A = np.array([[2, 3],\n",
    "              [1, 4]])\n",
    "b = np.array([8, 7])\n",
    "\n",
    "# Solve for x\n",
    "x = np.linalg.solve(A, b)\n",
    "\n",
    "print(\"System of equations:\")\n",
    "print(\"2x + 3y = 8\")\n",
    "print(\"x + 4y = 7\")\n",
    "print(f\"\\nSolution: x={x[0]:.2f}, y={x[1]:.2f}\")\n",
    "\n",
    "# Verify\n",
    "result = A @ x\n",
    "print(f\"\\nVerification: A @ x = {result} (should equal {b})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897582c2",
   "metadata": {},
   "source": [
    "### üéØ ML Example: Linear Regression (Normal Equation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c31c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression using Normal Equation: Œ∏ = (X^T X)^(-1) X^T y\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate synthetic data: y = 3 + 2x + noise\n",
    "X = np.random.rand(100, 1) * 10  # 100 samples, 1 feature\n",
    "y = 3 + 2 * X + np.random.randn(100, 1) * 2  # y = 3 + 2x + noise\n",
    "\n",
    "# Add bias column (x0 = 1)\n",
    "X_b = np.c_[np.ones((100, 1)), X]  # Add column of 1s\n",
    "\n",
    "# Normal equation: Œ∏ = (X^T X)^(-1) X^T y\n",
    "theta = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y\n",
    "\n",
    "print(\"Linear Regression (Normal Equation)\")\n",
    "print(f\"\\nTrue equation: y = 3 + 2x\")\n",
    "print(f\"Learned: y = {theta[0][0]:.2f} + {theta[1][0]:.2f}x\")\n",
    "print(f\"\\nParameters: intercept={theta[0][0]:.2f}, slope={theta[1][0]:.2f}\")\n",
    "\n",
    "# Make prediction\n",
    "X_new = np.array([[0], [5], [10]])\n",
    "X_new_b = np.c_[np.ones((3, 1)), X_new]\n",
    "y_pred = X_new_b @ theta\n",
    "\n",
    "print(\"\\nPredictions:\")\n",
    "for i in range(len(X_new)):\n",
    "    print(f\"  x={X_new[i][0]:.1f} ‚Üí y={y_pred[i][0]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ae85ca",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5Ô∏è‚É£ Eigenvalues and Eigenvectors\n",
    "\n",
    "Important for PCA and dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d629db14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eigenvalues and eigenvectors\n",
    "A = np.array([[4, 2],\n",
    "              [1, 3]])\n",
    "\n",
    "eigenvalues, eigenvectors = np.linalg.eig(A)\n",
    "\n",
    "print(\"Matrix A:\")\n",
    "print(A)\n",
    "print(f\"\\nEigenvalues: {eigenvalues}\")\n",
    "print(\"\\nEigenvectors:\")\n",
    "print(eigenvectors)\n",
    "\n",
    "# Verify: A * v = Œª * v\n",
    "v1 = eigenvectors[:, 0]\n",
    "lambda1 = eigenvalues[0]\n",
    "\n",
    "print(f\"\\nVerification for first eigenvector:\")\n",
    "print(f\"A @ v1 = {A @ v1}\")\n",
    "print(f\"Œª1 * v1 = {lambda1 * v1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5c5586",
   "metadata": {},
   "source": [
    "### üéØ ML Example: Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35cdf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple PCA implementation\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate correlated data\n",
    "X = np.random.randn(100, 2)\n",
    "X[:, 1] = X[:, 0] * 2 + np.random.randn(100) * 0.5\n",
    "\n",
    "print(\"Original data shape:\", X.shape)\n",
    "\n",
    "# Step 1: Center the data\n",
    "X_centered = X - np.mean(X, axis=0)\n",
    "\n",
    "# Step 2: Compute covariance matrix\n",
    "cov_matrix = np.cov(X_centered.T)\n",
    "print(\"\\nCovariance matrix:\")\n",
    "print(cov_matrix)\n",
    "\n",
    "# Step 3: Compute eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "\n",
    "print(\"\\nEigenvalues (variance explained):\", eigenvalues)\n",
    "print(\"Eigenvectors (principal components):\")\n",
    "print(eigenvectors)\n",
    "\n",
    "# Step 4: Sort by eigenvalues (descending)\n",
    "idx = eigenvalues.argsort()[::-1]\n",
    "eigenvalues = eigenvalues[idx]\n",
    "eigenvectors = eigenvectors[:, idx]\n",
    "\n",
    "# Explained variance ratio\n",
    "explained_var = eigenvalues / np.sum(eigenvalues)\n",
    "print(f\"\\nExplained variance ratio:\")\n",
    "print(f\"  PC1: {explained_var[0]:.2%}\")\n",
    "print(f\"  PC2: {explained_var[1]:.2%}\")\n",
    "\n",
    "# Step 5: Project onto first principal component\n",
    "X_pca = X_centered @ eigenvectors[:, 0].reshape(-1, 1)\n",
    "print(f\"\\nProjected data shape: {X_pca.shape} (reduced from 2D to 1D)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4286e6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6Ô∏è‚É£ Matrix Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a23e779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Singular Value Decomposition (SVD)\n",
    "A = np.array([[1, 2, 3],\n",
    "              [4, 5, 6],\n",
    "              [7, 8, 9]])\n",
    "\n",
    "U, s, VT = np.linalg.svd(A)\n",
    "\n",
    "print(\"Matrix A:\")\n",
    "print(A)\n",
    "print(f\"\\nU shape: {U.shape}\")\n",
    "print(f\"s (singular values): {s}\")\n",
    "print(f\"VT shape: {VT.shape}\")\n",
    "\n",
    "# Reconstruct A from SVD\n",
    "S = np.zeros((3, 3))\n",
    "np.fill_diagonal(S, s)\n",
    "A_reconstructed = U @ S @ VT\n",
    "\n",
    "print(\"\\nReconstructed A:\")\n",
    "print(np.round(A_reconstructed, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a139ad",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Practice Exercises\n",
    "\n",
    "### Exercise 1: Batch Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0166d299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate forward pass for a batch of 32 samples\n",
    "np.random.seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "input_dim = 10\n",
    "output_dim = 5\n",
    "\n",
    "# Input batch\n",
    "X = np.random.randn(batch_size, input_dim)\n",
    "\n",
    "# Weights and bias\n",
    "W = np.random.randn(input_dim, output_dim)\n",
    "b = np.random.randn(output_dim)\n",
    "\n",
    "# Your code: compute output = X @ W + b\n",
    "output = X @ W + b\n",
    "\n",
    "print(f\"Input shape: {X.shape}\")\n",
    "print(f\"Weights shape: {W.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nFirst sample output: {output[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d51a1dc",
   "metadata": {},
   "source": [
    "### Exercise 2: Normalize Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453b256f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize each row to unit length\n",
    "vectors = np.array([[3, 4],\n",
    "                    [5, 12],\n",
    "                    [8, 15]])\n",
    "\n",
    "# Your code: normalize each row\n",
    "norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "normalized = vectors / norms\n",
    "\n",
    "print(\"Original vectors:\")\n",
    "print(vectors)\n",
    "print(\"\\nNormalized vectors:\")\n",
    "print(normalized)\n",
    "print(\"\\nVerify lengths (should all be 1):\")\n",
    "print(np.linalg.norm(normalized, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25b8237",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "‚úÖ **Matrix Multiplication**: @ operator, np.dot(), np.matmul()  \n",
    "‚úÖ **Dot Products**: Vector operations, magnitude, unit vectors  \n",
    "‚úÖ **Matrix Properties**: Transpose, determinant, inverse  \n",
    "‚úÖ **Linear Equations**: Solving Ax=b, normal equation for regression  \n",
    "‚úÖ **Eigenanalysis**: Eigenvalues, eigenvectors, PCA  \n",
    "‚úÖ **Decomposition**: SVD for matrix factorization  \n",
    "‚úÖ **ML Applications**: Neural networks, regression, PCA, similarity\n",
    "\n",
    "### üöÄ Next Steps\n",
    "\n",
    "Continue to:\n",
    "- **`pandas_dataframes_basics.ipynb`** - Data manipulation with Pandas\n",
    "- Apply these concepts in scikit-learn algorithms\n",
    "\n",
    "---\n",
    "\n",
    "**Course:** MLM-101 - Machine Learning Mastery  \n",
    "**Website:** [https://flowdiary.com.ng/course/MLM-101](https://flowdiary.com.ng/course/MLM-101)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
