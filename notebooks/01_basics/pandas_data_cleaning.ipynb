{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8a14483",
   "metadata": {},
   "source": [
    "# Pandas Data Cleaning\n",
    "\n",
    "**Course:** MLM-101 - Machine Learning Mastery  \n",
    "**Phase 4:** Pandas for Data Analysis (Lectures 35-36)  \n",
    "**Topics:** Missing Values, Duplicates, Data Type Conversion\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“š Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "âœ… Detect and handle missing values  \n",
    "âœ… Identify and remove duplicate rows  \n",
    "âœ… Convert data types appropriately  \n",
    "âœ… Handle outliers and inconsistent data  \n",
    "âœ… Prepare clean datasets for ML models\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b41106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d8c054",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ Handling Missing Values\n",
    "\n",
    "Missing data is one of the most common data quality issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8c1e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset with missing values\n",
    "df = pd.DataFrame({\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve', 'Frank'],\n",
    "    'Age': [25, np.nan, 35, 28, np.nan, 45],\n",
    "    'Salary': [50000, 60000, np.nan, 55000, 65000, np.nan],\n",
    "    'Department': ['Engineering', 'Marketing', None, 'Sales', 'Engineering', 'Marketing']\n",
    "})\n",
    "\n",
    "print(\"Dataset with missing values:\")\n",
    "print(df)\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dd49ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect missing values\n",
    "print(\"Missing values (True/False):\")\n",
    "print(df.isnull())\n",
    "\n",
    "print(\"\\nMissing count per column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"\\nMissing percentage per column:\")\n",
    "print((df.isnull().sum() / len(df) * 100).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cc4359",
   "metadata": {},
   "source": [
    "### Strategy 1: Drop Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b63a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with ANY missing value\n",
    "df_dropna = df.dropna()\n",
    "print(\"After dropping rows with ANY missing value:\")\n",
    "print(df_dropna)\n",
    "print(f\"Shape: {df.shape} â†’ {df_dropna.shape}\")\n",
    "\n",
    "# Drop rows where ALL values are missing\n",
    "df_dropna_all = df.dropna(how='all')\n",
    "print(\"\\nAfter dropping rows where ALL values are missing:\")\n",
    "print(df_dropna_all)\n",
    "\n",
    "# Drop columns with missing values\n",
    "df_dropna_cols = df.dropna(axis=1)\n",
    "print(\"\\nAfter dropping columns with missing values:\")\n",
    "print(df_dropna_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23df9388",
   "metadata": {},
   "source": [
    "### Strategy 2: Fill Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ff945e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill with specific value\n",
    "df_fill_zero = df.fillna(0)\n",
    "print(\"Fill with 0:\")\n",
    "print(df_fill_zero)\n",
    "\n",
    "# Fill with mean (numerical columns)\n",
    "df_fill_mean = df.copy()\n",
    "df_fill_mean['Age'] = df_fill_mean['Age'].fillna(df_fill_mean['Age'].mean())\n",
    "df_fill_mean['Salary'] = df_fill_mean['Salary'].fillna(df_fill_mean['Salary'].mean())\n",
    "print(\"\\nFill numerical with mean:\")\n",
    "print(df_fill_mean)\n",
    "\n",
    "# Fill with median\n",
    "df_fill_median = df.copy()\n",
    "df_fill_median['Age'] = df_fill_median['Age'].fillna(df_fill_median['Age'].median())\n",
    "print(\"\\nFill Age with median:\")\n",
    "print(df_fill_median[['Name', 'Age']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9bd632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill with mode (most frequent)\n",
    "df_fill_mode = df.copy()\n",
    "df_fill_mode['Department'] = df_fill_mode['Department'].fillna(df_fill_mode['Department'].mode()[0])\n",
    "print(\"Fill Department with mode:\")\n",
    "print(df_fill_mode[['Name', 'Department']])\n",
    "\n",
    "# Forward fill (use previous value)\n",
    "df_ffill = df.fillna(method='ffill')\n",
    "print(\"\\nForward fill:\")\n",
    "print(df_ffill)\n",
    "\n",
    "# Backward fill (use next value)\n",
    "df_bfill = df.fillna(method='bfill')\n",
    "print(\"\\nBackward fill:\")\n",
    "print(df_bfill)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630dfd5d",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ ML Example: Smart Imputation Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b64363e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML-ready imputation strategy\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Numerical: Fill with median (robust to outliers)\n",
    "for col in df_clean.select_dtypes(include=[np.number]).columns:\n",
    "    df_clean[col] = df_clean[col].fillna(df_clean[col].median())\n",
    "\n",
    "# Categorical: Fill with mode or 'Unknown'\n",
    "for col in df_clean.select_dtypes(include=['object']).columns:\n",
    "    if df_clean[col].mode().empty:\n",
    "        df_clean[col] = df_clean[col].fillna('Unknown')\n",
    "    else:\n",
    "        df_clean[col] = df_clean[col].fillna(df_clean[col].mode()[0])\n",
    "\n",
    "print(\"Clean dataset:\")\n",
    "print(df_clean)\n",
    "print(\"\\nMissing values:\")\n",
    "print(df_clean.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66b6bbd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2ï¸âƒ£ Handling Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f98930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset with duplicates\n",
    "df_dup = pd.DataFrame({\n",
    "    'Name': ['Alice', 'Bob', 'Alice', 'Charlie', 'Bob', 'David'],\n",
    "    'Score': [85, 92, 85, 78, 92, 95],\n",
    "    'Subject': ['Math', 'Science', 'Math', 'Math', 'Science', 'Math']\n",
    "})\n",
    "\n",
    "print(\"Dataset with duplicates:\")\n",
    "print(df_dup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5066c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect duplicates\n",
    "print(\"Duplicate rows (True/False):\")\n",
    "print(df_dup.duplicated())\n",
    "\n",
    "print(\"\\nNumber of duplicates:\", df_dup.duplicated().sum())\n",
    "\n",
    "print(\"\\nDuplicate rows:\")\n",
    "print(df_dup[df_dup.duplicated()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432b009b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates (keep first occurrence)\n",
    "df_no_dup = df_dup.drop_duplicates()\n",
    "print(\"After removing duplicates:\")\n",
    "print(df_no_dup)\n",
    "print(f\"Shape: {df_dup.shape} â†’ {df_no_dup.shape}\")\n",
    "\n",
    "# Remove duplicates (keep last occurrence)\n",
    "df_no_dup_last = df_dup.drop_duplicates(keep='last')\n",
    "print(\"\\nKeep last occurrence:\")\n",
    "print(df_no_dup_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049f25c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates based on specific columns\n",
    "df_no_dup_name = df_dup.drop_duplicates(subset=['Name'])\n",
    "print(\"Remove duplicates based on Name only:\")\n",
    "print(df_no_dup_name)\n",
    "\n",
    "df_no_dup_cols = df_dup.drop_duplicates(subset=['Name', 'Subject'])\n",
    "print(\"\\nRemove duplicates based on Name and Subject:\")\n",
    "print(df_no_dup_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edd2ce0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3ï¸âƒ£ Data Type Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ad4f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset with wrong types\n",
    "df_types = pd.DataFrame({\n",
    "    'ID': ['1', '2', '3', '4', '5'],\n",
    "    'Age': ['25', '30', '35', '28', '32'],\n",
    "    'Salary': ['50000', '60000', '70000', '55000', '65000'],\n",
    "    'Active': ['True', 'False', 'True', 'True', 'False']\n",
    "})\n",
    "\n",
    "print(\"Original data types:\")\n",
    "print(df_types.dtypes)\n",
    "print(\"\\nDataset:\")\n",
    "print(df_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c00b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to appropriate types\n",
    "df_types['ID'] = df_types['ID'].astype(int)\n",
    "df_types['Age'] = df_types['Age'].astype(int)\n",
    "df_types['Salary'] = df_types['Salary'].astype(float)\n",
    "df_types['Active'] = df_types['Active'].astype(bool)\n",
    "\n",
    "print(\"After type conversion:\")\n",
    "print(df_types.dtypes)\n",
    "print(\"\\nDataset:\")\n",
    "print(df_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cf82c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to categorical (saves memory for repeated values)\n",
    "df_cat = pd.DataFrame({\n",
    "    'Department': ['Engineering', 'Marketing', 'Engineering', 'Sales', 'Engineering', 'Marketing'] * 100\n",
    "})\n",
    "\n",
    "print(f\"Object type memory: {df_cat['Department'].memory_usage(deep=True)} bytes\")\n",
    "\n",
    "df_cat['Department'] = df_cat['Department'].astype('category')\n",
    "print(f\"Category type memory: {df_cat['Department'].memory_usage(deep=True)} bytes\")\n",
    "print(f\"Memory saved: {((1 - df_cat['Department'].memory_usage(deep=True) / pd.Series(df_cat['Department'].astype('object')).memory_usage(deep=True)) * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621a7a78",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4ï¸âƒ£ Handling Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5fe631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset with outliers\n",
    "np.random.seed(42)\n",
    "data = np.random.normal(100, 15, 100).tolist()\n",
    "data.extend([200, 250, -50])  # Add outliers\n",
    "\n",
    "df_outliers = pd.DataFrame({'Value': data})\n",
    "\n",
    "print(\"Statistical summary:\")\n",
    "print(df_outliers['Value'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524d1e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: IQR (Interquartile Range)\n",
    "Q1 = df_outliers['Value'].quantile(0.25)\n",
    "Q3 = df_outliers['Value'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "print(f\"Lower bound: {lower_bound:.2f}\")\n",
    "print(f\"Upper bound: {upper_bound:.2f}\")\n",
    "\n",
    "# Filter outliers\n",
    "df_no_outliers = df_outliers[(df_outliers['Value'] >= lower_bound) & \n",
    "                              (df_outliers['Value'] <= upper_bound)]\n",
    "\n",
    "print(f\"\\nOriginal size: {len(df_outliers)}\")\n",
    "print(f\"After removing outliers: {len(df_no_outliers)}\")\n",
    "print(f\"Outliers removed: {len(df_outliers) - len(df_no_outliers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d3be41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Z-score (for normally distributed data)\n",
    "from scipy import stats\n",
    "\n",
    "df_outliers['z_score'] = np.abs(stats.zscore(df_outliers['Value']))\n",
    "df_no_outliers_z = df_outliers[df_outliers['z_score'] < 3]\n",
    "\n",
    "print(f\"Using Z-score (threshold=3):\")\n",
    "print(f\"Outliers removed: {len(df_outliers) - len(df_no_outliers_z)}\")\n",
    "print(f\"\\nRemaining statistics:\")\n",
    "print(df_no_outliers_z['Value'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677aa04b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5ï¸âƒ£ String Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6960b7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create messy string data\n",
    "df_strings = pd.DataFrame({\n",
    "    'Name': ['  Alice  ', 'BOB', 'charlie', ' David', 'Eve   '],\n",
    "    'Email': ['alice@EXAMPLE.com', 'bob@example.COM', 'charlie@Example.com', 'david@example.com', 'eve@EXAMPLE.COM']\n",
    "})\n",
    "\n",
    "print(\"Original:\")\n",
    "print(df_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4ed543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean strings\n",
    "df_strings['Name'] = df_strings['Name'].str.strip()  # Remove whitespace\n",
    "df_strings['Name'] = df_strings['Name'].str.title()  # Title case\n",
    "df_strings['Email'] = df_strings['Email'].str.lower()  # Lowercase\n",
    "\n",
    "print(\"After cleaning:\")\n",
    "print(df_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be9cc25",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ¯ Practice Exercise: Complete Data Cleaning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b91aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create messy dataset\n",
    "messy_data = pd.DataFrame({\n",
    "    'customer_id': ['1', '2', '3', '3', '4', '5', '6'],\n",
    "    'name': ['  Alice  ', 'BOB', 'charlie', 'charlie', ' David', 'Eve', 'Frank'],\n",
    "    'age': [25, np.nan, '35', '35', 28, np.nan, 45],\n",
    "    'salary': [50000, 60000, np.nan, np.nan, 55000, 65000, 150000],\n",
    "    'department': ['Engineering', 'marketing', None, None, 'Sales', 'Engineering', 'Engineering']\n",
    "})\n",
    "\n",
    "print(\"Messy dataset:\")\n",
    "print(messy_data)\n",
    "print(\"\\nData types:\")\n",
    "print(messy_data.dtypes)\n",
    "print(\"\\nMissing values:\")\n",
    "print(messy_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be093f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning pipeline\n",
    "clean_data = messy_data.copy()\n",
    "\n",
    "# 1. Convert types\n",
    "clean_data['customer_id'] = clean_data['customer_id'].astype(int)\n",
    "clean_data['age'] = pd.to_numeric(clean_data['age'], errors='coerce')\n",
    "\n",
    "# 2. Clean strings\n",
    "clean_data['name'] = clean_data['name'].str.strip().str.title()\n",
    "clean_data['department'] = clean_data['department'].str.title()\n",
    "\n",
    "# 3. Handle missing values\n",
    "clean_data['age'] = clean_data['age'].fillna(clean_data['age'].median())\n",
    "clean_data['salary'] = clean_data['salary'].fillna(clean_data['salary'].median())\n",
    "clean_data['department'] = clean_data['department'].fillna('Unknown')\n",
    "\n",
    "# 4. Remove duplicates\n",
    "clean_data = clean_data.drop_duplicates()\n",
    "\n",
    "# 5. Handle outliers (salary > 100k might be outlier in this small dataset)\n",
    "Q1 = clean_data['salary'].quantile(0.25)\n",
    "Q3 = clean_data['salary'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "clean_data = clean_data[(clean_data['salary'] >= Q1 - 1.5*IQR) & \n",
    "                        (clean_data['salary'] <= Q3 + 1.5*IQR)]\n",
    "\n",
    "print(\"Clean dataset:\")\n",
    "print(clean_data)\n",
    "print(\"\\nData types:\")\n",
    "print(clean_data.dtypes)\n",
    "print(\"\\nMissing values:\")\n",
    "print(clean_data.isnull().sum())\n",
    "print(f\"\\nRows: {len(messy_data)} â†’ {len(clean_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb422af0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ“ Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "âœ… **Missing Values**: Detection, dropping, filling strategies (mean/median/mode/forward/backward)  \n",
    "âœ… **Duplicates**: Identify and remove duplicate rows  \n",
    "âœ… **Data Types**: Convert strings to numbers, booleans, categories  \n",
    "âœ… **Outliers**: IQR and Z-score methods for detection and removal  \n",
    "âœ… **String Cleaning**: Strip whitespace, case conversion  \n",
    "âœ… **ML Pipeline**: Complete data cleaning workflow\n",
    "\n",
    "### ðŸš€ Next Steps\n",
    "\n",
    "Continue to:\n",
    "- **`pandas_data_analysis.ipynb`** - GroupBy, aggregations, pivot tables\n",
    "- Apply cleaning to real ML datasets\n",
    "\n",
    "---\n",
    "\n",
    "**Course:** MLM-101 - Machine Learning Mastery  \n",
    "**Website:** [https://flowdiary.com.ng/course/MLM-101](https://flowdiary.com.ng/course/MLM-101)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
