"""
RAG with LangChain and Gradio - Lecture 83
Deployment notebook demonstrating retrieval-augmented generation
"""

import gradio as gr
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss
from typing import List, Tuple
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class SimpleRAGSystem:
    """Simple RAG system using FAISS and sentence transformers."""
    
    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):
        """Initialize RAG system with embedding model."""
        logger.info(f"Loading embedding model: {model_name}")
        self.embedder = SentenceTransformer(model_name)
        self.dimension = self.embedder.get_sentence_embedding_dimension()
        self.index = None
        self.documents = []
        
    def build_index(self, documents: List[str]):
        """Build FAISS index from documents."""
        logger.info(f"Building index for {len(documents)} documents...")
        self.documents = documents
        
        # Generate embeddings
        embeddings = self.embedder.encode(documents, show_progress_bar=True)
        embeddings = np.array(embeddings).astype('float32')
        
        # Create FAISS index
        self.index = faiss.IndexFlatL2(self.dimension)
        self.index.add(embeddings)
        
        logger.info(f"âœ“ Index built with {self.index.ntotal} vectors")
        
    def retrieve(self, query: str, top_k: int = 3) -> List[Tuple[str, float]]:
        """Retrieve top-k most relevant documents."""
        if self.index is None:
            raise ValueError("Index not built. Call build_index() first.")
        
        # Embed query
        query_embedding = self.embedder.encode([query]).astype('float32')
        
        # Search
        distances, indices = self.index.search(query_embedding, top_k)
        
        # Format results
        results = []
        for idx, dist in zip(indices[0], distances[0]):
            if idx < len(self.documents):
                results.append((self.documents[idx], float(dist)))
        
        return results
    
    def generate_answer(self, query: str, context: List[str]) -> str:
        """
        Generate answer using context.
        In production, this would call an LLM API.
        """
        # Mock generation (replace with actual LLM call)
        context_text = "\n\n".join(context[:3])
        
        answer = f"""Based on the retrieved context, here's my response to: "{query}"

Retrieved Context:
{context_text}

Answer: This is a placeholder answer. In production, this would be generated by an LLM like GPT-4, Claude, or an open-source model, using the context above to provide an informed response.

To use a real LLM, set your API key and uncomment the LLM integration code in the app.
"""
        return answer
    
    def query(self, question: str, top_k: int = 3) -> Tuple[str, List[str]]:
        """Complete RAG pipeline: retrieve + generate."""
        # Retrieve relevant documents
        retrieved = self.retrieve(question, top_k)
        sources = [doc for doc, _ in retrieved]
        
        # Generate answer
        answer = self.generate_answer(question, sources)
        
        return answer, sources


# Sample corpus for demonstration
SAMPLE_DOCUMENTS = [
    "Deep learning is a subset of machine learning that uses neural networks with multiple layers. "
    "It has revolutionized computer vision, natural language processing, and many other fields.",
    
    "Model deployment involves taking a trained machine learning model and making it available for "
    "predictions in a production environment. This includes serialization, API creation, and monitoring.",
    
    "FastAPI is a modern, fast web framework for building APIs with Python. It's ideal for serving "
    "machine learning models due to its high performance and automatic data validation.",
    
    "Docker containers package applications and their dependencies together, ensuring consistent "
    "behavior across different environments. They're essential for deploying ML models reliably.",
    
    "RAG (Retrieval-Augmented Generation) combines information retrieval with text generation. "
    "It retrieves relevant documents and uses them as context for generating accurate responses.",
    
    "TensorFlow and PyTorch are the two most popular deep learning frameworks. TensorFlow offers "
    "production-ready tools like TensorFlow Serving, while PyTorch is favored for research.",
]


def create_gradio_interface():
    """Create Gradio interface for RAG system."""
    
    # Initialize RAG system
    rag = SimpleRAGSystem()
    rag.build_index(SAMPLE_DOCUMENTS)
    
    def answer_question(question: str, num_sources: int = 3):
        """Process question through RAG pipeline."""
        if not question.strip():
            return "Please enter a question.", ""
        
        try:
            answer, sources = rag.query(question, top_k=num_sources)
            
            # Format sources
            sources_text = "\n\n".join([
                f"ðŸ“„ Source {i+1}:\n{src}"
                for i, src in enumerate(sources)
            ])
            
            return answer, sources_text
            
        except Exception as e:
            logger.error(f"Error processing question: {e}")
            return f"Error: {str(e)}", ""
    
    # Create Gradio interface
    with gr.Blocks(title="RAG Demo - Lecture 83") as demo:
        gr.Markdown("# ðŸ¤– RAG System Demo - Deployment Lecture")
        gr.Markdown(
            "Ask questions about deep learning deployment! "
            "This system retrieves relevant information and generates answers."
        )
        
        with gr.Row():
            with gr.Column():
                question_input = gr.Textbox(
                    label="Your Question",
                    placeholder="e.g., What is model deployment?",
                    lines=3
                )
                num_sources = gr.Slider(
                    minimum=1,
                    maximum=5,
                    value=3,
                    step=1,
                    label="Number of sources to retrieve"
                )
                submit_btn = gr.Button("Ask Question", variant="primary")
            
            with gr.Column():
                answer_output = gr.Textbox(
                    label="Answer",
                    lines=10,
                    interactive=False
                )
                sources_output = gr.Textbox(
                    label="Retrieved Sources",
                    lines=8,
                    interactive=False
                )
        
        # Example questions
        gr.Examples(
            examples=[
                ["What is deep learning?"],
                ["How do I deploy a machine learning model?"],
                ["What is RAG and how does it work?"],
                ["Tell me about Docker containers"],
            ],
            inputs=question_input
        )
        
        # Connect button to function
        submit_btn.click(
            fn=answer_question,
            inputs=[question_input, num_sources],
            outputs=[answer_output, sources_output]
        )
    
    return demo


if __name__ == "__main__":
    demo = create_gradio_interface()
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False
    )
